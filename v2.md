# Production-Ready DevOps Infrastructure Guide

## Архитектура и Топология

```
Internet
    │
    ├─→ Router (10.0.10.1)
    │
    └─→ Proxmox Cluster (3 nodes recommended)
            │
            ├─→ Management VLAN (10.0.10.0/24)
            │   └─→ Bastion/Jump Host: 10.0.10.30
            │
            ├─→ Infrastructure VLAN (192.168.10.0/24)
            │   ├─→ GitLab Primary: 192.168.10.10
            │   ├─→ GitLab Secondary (HA): 192.168.10.11
            │   ├─→ Nexus: 192.168.10.20
            │   ├─→ SonarQube: 192.168.10.30
            │   ├─→ PostgreSQL (shared): 192.168.10.40
            │   └─→ NFS/Ceph Storage: 192.168.10.50-52
            │
            ├─→ Kubernetes VLAN (192.168.20.0/24)
            │   ├─→ K3s Master-1: 192.168.20.10
            │   ├─→ K3s Master-2: 192.168.20.11 (HA)
            │   ├─→ K3s Master-3: 192.168.20.12 (HA)
            │   ├─→ K3s Worker-1: 192.168.20.21
            │   ├─→ K3s Worker-2: 192.168.20.22
            │   ├─→ K3s Worker-3: 192.168.20.23
            │   └─→ MetalLB Pool: 192.168.20.100-150
            │
            └─→ DMZ VLAN (192.168.30.0/24)
                ├─→ HAProxy-1: 192.168.30.10 (VRRP Master)
                ├─→ HAProxy-2: 192.168.30.11 (VRRP Backup)
                └─→ VIP: 192.168.30.1 (Keepalived)
```
<img width="1024" height="1536" alt="image" src="https://github.com/user-attachments/assets/7b1157fb-0d47-4710-92aa-6d91da9f7aff" />

---

## Этап 1: Подготовка Базовой Инфраструктуры

### 1.1 Настройка VLAN на Proxmox

bash

```bash
# На каждом узле Proxmox создаем VLAN интерфейсы
# /etc/network/interfaces

auto vmbr0
iface vmbr0 inet static
    address 10.0.10.0/24
    bridge-ports eth0
    bridge-stp off
    bridge-fd 0

# Management VLAN
auto vmbr0.10
iface vmbr0.10 inet static
    address 10.0.10.0/24
    vlan-raw-device vmbr0

# Infrastructure VLAN
auto vmbr0.192
iface vmbr0.192 inet static
    address 192.168.10.0/24
    vlan-raw-device vmbr0

# Kubernetes VLAN
auto vmbr0.20
iface vmbr0.20 inet static
    address 192.168.20.0/24
    vlan-raw-device vmbr0

# DMZ VLAN
auto vmbr0.30
iface vmbr0.30 inet static
    address 192.168.30.0/24
    vlan-raw-device vmbr0

# Перезапуск сети
systemctl restart networking
```

### 1.2 Создание NFS Storage для Shared Data

bash

```bash
# На выделенном NFS сервере (192.168.10.50)
apt update && apt install nfs-kernel-server -y

# Создаем директории
mkdir -p /export/gitlab-data
mkdir -p /export/nexus-data
mkdir -p /export/sonarqube-data
mkdir -p /export/k8s-pv

# Настраиваем /etc/exports
cat << EOF >> /etc/exports
/export/gitlab-data 192.168.10.0/24(rw,sync,no_subtree_check,no_root_squash)
/export/nexus-data 192.168.10.0/24(rw,sync,no_subtree_check,no_root_squash)
/export/sonarqube-data 192.168.10.0/24(rw,sync,no_subtree_check,no_root_squash)
/export/k8s-pv 192.168.20.0/24(rw,sync,no_subtree_check,no_root_squash)
EOF

# Применяем изменения
exportfs -arv
systemctl enable --now nfs-server
```

---

## Этап 2: Развертывание Stateful Infrastructure Services

### 2.1 PostgreSQL High Availability Cluster

bash

```bash
# VM: postgresql-primary (192.168.10.40)
# Specs: 4 vCPU, 8GB RAM, 100GB SSD

# Установка PostgreSQL 15
apt update && apt install -y postgresql-15 postgresql-contrib-15

# Настройка для HA /etc/postgresql/15/main/postgresql.conf
cat << EOF >> /etc/postgresql/15/main/postgresql.conf
listen_addresses = '*'
wal_level = replica
max_wal_senders = 10
max_replication_slots = 10
hot_standby = on
archive_mode = on
archive_command = 'test ! -f /var/lib/postgresql/15/archive/%f && cp %p /var/lib/postgresql/15/archive/%f'
EOF

# Настройка pg_hba.conf
cat << EOF >> /etc/postgresql/15/main/pg_hba.conf
host    replication     replicator      192.168.10.0/24         scram-sha-256
host    all             all             192.168.10.0/24         scram-sha-256
host    all             all             192.168.20.0/24         scram-sha-256
EOF

# Создаем пользователя для репликации
sudo -u postgres psql << EOF
CREATE USER replicator WITH REPLICATION ENCRYPTED PASSWORD 'strong_password';
CREATE DATABASE gitlab_production;
CREATE DATABASE sonarqube;
CREATE USER gitlab WITH ENCRYPTED PASSWORD 'gitlab_pass';
CREATE USER sonarqube WITH ENCRYPTED PASSWORD 'sonar_pass';
GRANT ALL PRIVILEGES ON DATABASE gitlab_production TO gitlab;
GRANT ALL PRIVILEGES ON DATABASE sonarqube TO sonarqube;
EOF

systemctl restart postgresql
```

### 2.2 GitLab Installation (Primary)

bash

```bash
# VM: gitlab-primary (192.168.10.10)
# Specs: 8 vCPU, 16GB RAM, 200GB SSD

# Монтируем NFS для данных
apt install -y nfs-common
mkdir -p /var/opt/gitlab
echo "192.168.10.50:/export/gitlab-data /var/opt/gitlab nfs defaults 0 0" >> /etc/fstab
mount -a

# Установка GitLab
curl https://packages.gitlab.com/install/repositories/gitlab/gitlab-ee/script.deb.sh | bash
EXTERNAL_URL="https://gitlab.yourdomain.com" apt-get install -y gitlab-ee

# Настройка /etc/gitlab/gitlab.rb
cat << 'EOF' > /etc/gitlab/gitlab.rb
external_url 'https://gitlab.yourdomain.com'

# PostgreSQL external
postgresql['enable'] = false
gitlab_rails['db_adapter'] = 'postgresql'
gitlab_rails['db_encoding'] = 'unicode'
gitlab_rails['db_host'] = '192.168.10.40'
gitlab_rails['db_port'] = 5432
gitlab_rails['db_database'] = 'gitlab_production'
gitlab_rails['db_username'] = 'gitlab'
gitlab_rails['db_password'] = 'gitlab_pass'

# Redis внутренний (или можно вынести)
redis['enable'] = true

# GitLab Runner integration
gitlab_rails['gitlab_shell_ssh_port'] = 22

# Backup settings
gitlab_rails['backup_path'] = "/var/opt/gitlab/backups"
gitlab_rails['backup_keep_time'] = 604800

# Email settings (опционально)
gitlab_rails['smtp_enable'] = true
gitlab_rails['smtp_address'] = "smtp.gmail.com"
gitlab_rails['smtp_port'] = 587
gitlab_rails['smtp_user_name'] = "your-email@gmail.com"
gitlab_rails['smtp_password'] = "your-password"
gitlab_rails['smtp_domain'] = "smtp.gmail.com"
gitlab_rails['smtp_authentication'] = "login"
gitlab_rails['smtp_enable_starttls_auto'] = true
gitlab_rails['gitlab_email_from'] = 'gitlab@yourdomain.com'

# Monitoring
prometheus['enable'] = true
grafana['enable'] = true
EOF

# Применяем конфигурацию
gitlab-ctl reconfigure
gitlab-ctl restart

# Получаем initial root password
cat /etc/gitlab/initial_root_password
```

### 2.3 Nexus Repository Manager

bash

```bash
# VM: nexus (192.168.10.20)
# Specs: 4 vCPU, 8GB RAM, 500GB SSD

# Монтируем NFS
apt install -y nfs-common
mkdir -p /opt/sonatype-work
echo "192.168.10.50:/export/nexus-data /opt/sonatype-work nfs defaults 0 0" >> /etc/fstab
mount -a

# Установка Java 11
apt update && apt install -y openjdk-11-jdk

# Загрузка Nexus
cd /opt
wget https://download.sonatype.com/nexus/3/latest-unix.tar.gz
tar -xvf latest-unix.tar.gz
mv nexus-3.* nexus

# Создаем пользователя
useradd -r -m -U -d /opt/sonatype-work -s /bin/bash nexus
chown -R nexus:nexus /opt/nexus /opt/sonatype-work

# Systemd service
cat << 'EOF' > /etc/systemd/system/nexus.service
[Unit]
Description=Nexus Repository Manager
After=network.target

[Service]
Type=forking
LimitNOFILE=65536
ExecStart=/opt/nexus/bin/nexus start
ExecStop=/opt/nexus/bin/nexus stop
User=nexus
Restart=on-abort
TimeoutSec=600

[Install]
WantedBy=multi-user.target
EOF

# Настройка nexus.vmoptions для production
cat << EOF > /opt/nexus/bin/nexus.vmoptions
-Xms2703m
-Xmx2703m
-XX:MaxDirectMemorySize=2703m
-XX:+UnlockDiagnosticVMOptions
-XX:+LogVMOutput
-XX:LogFile=/opt/sonatype-work/nexus3/log/jvm.log
-XX:-OmitStackTraceInFastThrow
-Djava.net.preferIPv4Stack=true
-Dkaraf.home=.
-Dkaraf.base=.
-Dkaraf.etc=etc/karaf
-Djava.util.logging.config.file=etc/karaf/java.util.logging.properties
-Dkaraf.data=/opt/sonatype-work/nexus3
-Dkaraf.log=/opt/sonatype-work/nexus3/log
-Djava.io.tmpdir=/opt/sonatype-work/nexus3/tmp
EOF

systemctl daemon-reload
systemctl enable --now nexus

# Проверка
systemctl status nexus
# Nexus будет доступен на http://192.168.10.20:8081
# Initial admin password: cat /opt/sonatype-work/nexus3/admin.password
```

### 2.4 SonarQube Installation

bash

```bash
# VM: sonarqube (192.168.10.30)
# Specs: 4 vCPU, 8GB RAM, 100GB SSD

# Монтируем NFS
apt install -y nfs-common
mkdir -p /opt/sonarqube-data
echo "192.168.10.50:/export/sonarqube-data /opt/sonarqube-data nfs defaults 0 0" >> /etc/fstab
mount -a

# System tuning для SonarQube
cat << EOF >> /etc/sysctl.conf
vm.max_map_count=524288
fs.file-max=131072
EOF
sysctl -p

cat << EOF >> /etc/security/limits.conf
sonarqube   -   nofile   131072
sonarqube   -   nproc    8192
EOF

# Установка Java 17
apt update && apt install -y openjdk-17-jdk unzip

# Загрузка SonarQube
cd /opt
wget https://binaries.sonarsource.com/Distribution/sonarqube/sonarqube-10.3.0.82913.zip
unzip sonarqube-10.3.0.82913.zip
mv sonarqube-10.3.0.82913 sonarqube

# Создаем пользователя
useradd -r -m -U -d /opt/sonarqube -s /bin/bash sonarqube
chown -R sonarqube:sonarqube /opt/sonarqube /opt/sonarqube-data

# Настройка /opt/sonarqube/conf/sonar.properties
cat << EOF > /opt/sonarqube/conf/sonar.properties
sonar.jdbc.username=sonarqube
sonar.jdbc.password=sonar_pass
sonar.jdbc.url=jdbc:postgresql://192.168.10.40/sonarqube
sonar.web.host=0.0.0.0
sonar.web.port=9000
sonar.path.data=/opt/sonarqube-data/data
sonar.path.temp=/opt/sonarqube-data/temp
EOF

# Systemd service
cat << 'EOF' > /etc/systemd/system/sonarqube.service
[Unit]
Description=SonarQube service
After=network.target network-online.target postgresql.service
Requires=network-online.target

[Service]
Type=forking
ExecStart=/opt/sonarqube/bin/linux-x86-64/sonar.sh start
ExecStop=/opt/sonarqube/bin/linux-x86-64/sonar.sh stop
User=sonarqube
Group=sonarqube
Restart=on-failure
LimitNOFILE=131072
LimitNPROC=8192

[Install]
WantedBy=multi-user.target
EOF

systemctl daemon-reload
systemctl enable --now sonarqube

# Проверка
systemctl status sonarqube
# SonarQube доступен на http://192.168.10.30:9000
# Default credentials: admin/admin
```

---

## Этап 3: K3s Kubernetes Cluster Deployment

### 3.1 Подготовка всех K3s нод

bash

```bash
# Выполнить на ВСЕХ нодах (masters + workers)

# Отключаем swap
swapoff -a
sed -i '/swap/d' /etc/fstab

# Настройка системы
cat << EOF > /etc/modules-load.d/k3s.conf
overlay
br_netfilter
EOF

modprobe overlay
modprobe br_netfilter

cat << EOF > /etc/sysctl.d/k3s.conf
net.bridge.bridge-nf-call-iptables  = 1
net.bridge.bridge-nf-call-ip6tables = 1
net.ipv4.ip_forward                 = 1
EOF

sysctl --system

# Установка зависимостей
apt update
apt install -y curl wget nfs-common open-iscsi
```

### 3.2 Установка первого Master Node

bash

```bash
# На k3s-master-1 (192.168.20.10)

curl -sfL https://get.k3s.io | sh -s - server \
  --cluster-init \
  --tls-san=192.168.20.10 \
  --tls-san=192.168.20.1 \
  --disable traefik \
  --disable servicelb \
  --write-kubeconfig-mode=644 \
  --node-ip=192.168.20.10 \
  --advertise-address=192.168.20.10 \
  --flannel-iface=eth0

# Сохраняем токен для других нод
cat /var/lib/rancher/k3s/server/node-token

# Проверка
kubectl get nodes
```

### 3.3 Добавление дополнительных Master нод (HA)

bash

```bash
# На k3s-master-2 (192.168.20.11)
export K3S_TOKEN="<token-from-master-1>"

curl -sfL https://get.k3s.io | sh -s - server \
  --server https://192.168.20.10:6443 \
  --token=${K3S_TOKEN} \
  --tls-san=192.168.20.11 \
  --disable traefik \
  --disable servicelb \
  --write-kubeconfig-mode=644 \
  --node-ip=192.168.20.11

# На k3s-master-3 (192.168.20.12)
curl -sfL https://get.k3s.io | sh -s - server \
  --server https://192.168.20.10:6443 \
  --token=${K3S_TOKEN} \
  --tls-san=192.168.20.12 \
  --disable traefik \
  --disable servicelb \
  --write-kubeconfig-mode=644 \
  --node-ip=192.168.20.12
```

### 3.4 Добавление Worker нод

bash

```bash
# На всех worker нодах (192.168.20.21-23)
export K3S_TOKEN="<token-from-master-1>"
export K3S_URL="https://192.168.20.10:6443"

curl -sfL https://get.k3s.io | sh -s - agent \
  --server ${K3S_URL} \
  --token ${K3S_TOKEN} \
  --node-ip=192.168.20.2X

# Проверка на master-1
kubectl get nodes
# Должны увидеть все 6 нод (3 masters + 3 workers)
```

### 3.5 Установка MetalLB для LoadBalancer

bash

```bash
# На master-1
kubectl apply -f https://raw.githubusercontent.com/metallb/metallb/v0.13.12/config/manifests/metallb-native.yaml

# Ждем готовности
kubectl wait --namespace metallb-system \
  --for=condition=ready pod \
  --selector=app=metallb \
  --timeout=90s

# Создаем IP pool
cat << EOF | kubectl apply -f -
apiVersion: metallb.io/v1beta1
kind: IPAddressPool
metadata:
  name: default-pool
  namespace: metallb-system
spec:
  addresses:
  - 192.168.20.100-192.168.20.150
---
apiVersion: metallb.io/v1beta1
kind: L2Advertisement
metadata:
  name: default
  namespace: metallb-system
spec:
  ipAddressPools:
  - default-pool
EOF
```

### 3.6 Установка Longhorn для Persistent Storage

bash

```bash
# Установка зависимостей на всех нодах
apt install -y open-iscsi nfs-common

# На master-1
kubectl apply -f https://raw.githubusercontent.com/longhorn/longhorn/v1.5.3/deploy/longhorn.yaml

# Ждем готовности
kubectl -n longhorn-system get pods -w

# Создаем StorageClass по умолчанию
kubectl patch storageclass longhorn -p '{"metadata": {"annotations":{"storageclass.kubernetes.io/is-default-class":"true"}}}'

# Доступ к UI через port-forward
kubectl -n longhorn-system port-forward svc/longhorn-frontend 8080:80
```

### 3.7 Установка Ingress Nginx Controller

bash

```bash
# Установка Ingress Nginx
kubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/controller-v1.9.5/deploy/static/provider/cloud/deploy.yaml

# Проверка
kubectl get svc -n ingress-nginx
# Должен получить External IP из MetalLB pool

# Patch для использования MetalLB
kubectl patch svc ingress-nginx-controller -n ingress-nginx -p '{"spec":{"type":"LoadBalancer"}}'
```

---

## Этап 4: HAProxy + Keepalived для High Availability

### 4.1 Установка HAProxy на обоих нодах

bash

```bash
# На haproxy-1 (192.168.30.10) и haproxy-2 (192.168.30.11)

apt update && apt install -y haproxy keepalived

# Настройка /etc/haproxy/haproxy.cfg
cat << 'EOF' > /etc/haproxy/haproxy.cfg
global
    log /dev/log local0
    log /dev/log local1 notice
    chroot /var/lib/haproxy
    stats socket /run/haproxy/admin.sock mode 660 level admin
    stats timeout 30s
    user haproxy
    group haproxy
    daemon
    maxconn 4000

defaults
    log     global
    mode    http
    option  httplog
    option  dontlognull
    timeout connect 5000
    timeout client  50000
    timeout server  50000
    errorfile 400 /etc/haproxy/errors/400.http
    errorfile 403 /etc/haproxy/errors/403.http
    errorfile 408 /etc/haproxy/errors/408.http
    errorfile 500 /etc/haproxy/errors/500.http
    errorfile 502 /etc/haproxy/errors/502.http
    errorfile 503 /etc/haproxy/errors/503.http
    errorfile 504 /etc/haproxy/errors/504.http

# Stats page
listen stats
    bind *:8404
    stats enable
    stats uri /stats
    stats refresh 30s
    stats admin if TRUE

# GitLab HTTP
frontend gitlab_http
    bind *:80
    mode http
    default_backend gitlab_servers

backend gitlab_servers
    mode http
    balance roundrobin
    option httpchk GET /users/sign_in
    http-check expect status 200
    server gitlab-primary 192.168.10.10:80 check
    server gitlab-secondary 192.168.10.11:80 check backup

# GitLab HTTPS
frontend gitlab_https
    bind *:443
    mode tcp
    default_backend gitlab_https_servers

backend gitlab_https_servers
    mode tcp
    balance roundrobin
    option ssl-hello-chk
    server gitlab-primary 192.168.10.10:443 check
    server gitlab-secondary 192.168.10.11:443 check backup

# Nexus
frontend nexus_http
    bind *:8081
    mode http
    default_backend nexus_servers

backend nexus_servers
    mode http
    balance roundrobin
    option httpchk GET /
    server nexus 192.168.10.20:8081 check

# SonarQube
frontend sonarqube_http
    bind *:9000
    mode http
    default_backend sonarqube_servers

backend sonarqube_servers
    mode http
    balance roundrobin
    option httpchk GET /api/system/status
    server sonarqube 192.168.10.30:9000 check

# Kubernetes API
frontend k8s_api
    bind *:6443
    mode tcp
    default_backend k8s_api_servers

backend k8s_api_servers
    mode tcp
    balance roundrobin
    option tcp-check
    server k3s-master-1 192.168.20.10:6443 check
    server k3s-master-2 192.168.20.11:6443 check
    server k3s-master-3 192.168.20.12:6443 check

# Application Ingress (HTTP)
frontend app_http
    bind *:8080
    mode http
    default_backend ingress_http

backend ingress_http
    mode http
    balance roundrobin
    option httpchk GET /healthz
    server ingress-lb 192.168.20.100:80 check
EOF

systemctl restart haproxy
systemctl enable haproxy
```

### 4.2 Настройка Keepalived

bash

```bash
# На haproxy-1 (192.168.30.10) - MASTER
cat << 'EOF' > /etc/keepalived/keepalived.conf
vrrp_script chk_haproxy {
    script "/usr/bin/killall -0 haproxy"
    interval 2
    weight 2
}

vrrp_instance VI_1 {
    state MASTER
    interface eth0
    virtual_router_id 51
    priority 101
    advert_int 1
    
    authentication {
        auth_type PASS
        auth_pass SecurePassword123
    }
    
    virtual_ipaddress {
        192.168.30.1/24
    }
    
    track_script {
        chk_haproxy
    }
}
EOF

# На haproxy-2 (192.168.30.11) - BACKUP
cat << 'EOF' > /etc/keepalived/keepalived.conf
vrrp_script chk_haproxy {
    script "/usr/bin/killall -0 haproxy"
    interval 2
    weight 2
}

vrrp_instance VI_1 {
    state BACKUP
    interface eth0
    virtual_router_id 51
    priority 100
    advert_int 1
    
    authentication {
        auth_type PASS
        auth_pass SecurePassword123
    }
    
    virtual_ipaddress {
        192.168.30.1/24
    }
    
    track_script {
        chk_haproxy
    }
}
EOF

# На обоих нодах
systemctl restart keepalived
systemctl enable keepalived

# Проверка VIP
ip addr show eth0 | grep 192.168.30.1
```

---

## Этап 5: Настройка GitLab CI/CD Integration

### 5.1 Регистрация GitLab Runner в Kubernetes

bash

```bash
# Добавляем Helm repo
helm repo add gitlab https://charts.gitlab.io
helm repo update

# Получаем registration token из GitLab UI
# Settings -> CI/CD -> Runners -> Registration token

# Создаем namespace
kubectl create namespace gitlab-runner

# Устанавливаем GitLab Runner
helm install gitlab-runner gitlab/gitlab-runner \
  --namespace gitlab-runner \
  --set gitlabUrl=https://gitlab.yourdomain.com \
  --set runnerRegistrationToken="YOUR_REGISTRATION_TOKEN" \
  --set rbac.create=true \
  --set runners.privileged=true \
  --set runners.cache.cacheType=s3 \
  --set runners.config='
    [[runners]]
      [runners.kubernetes]
        namespace = "gitlab-runner"
        image = "ubuntu:22.04"
        privileged = true
        [[runners.kubernetes.volumes.empty_dir]]
          name = "docker-certs"
          mount_path = "/certs/client"
          medium = "Memory"
  '

# Проверка
kubectl get pods -n gitlab-runner
```

### 5.2 Создание тестового проекта (PetClinic)

bash

```bash
# На локальной машине или bastion host

# Клонируем проект
git clone https://github.com/spring-projects/spring-petclinic.git
cd spring-petclinic

# Настраиваем GitLab remote
git remote remove origin
git remote add origin https://gitlab.yourdomain.com/your-username/spring-petclinic.git

# Создаем .gitlab-ci.yml
cat << 'EOF' > .gitlab-ci.yml
stages:
  - build
  - test
  - quality
  - package
  - deploy

variables:
  MAVEN_OPTS: "-Dmaven.repo.local=$CI_PROJECT_DIR/.m2/repository"
  NEXUS_URL: "http://192.168.10.20:8081"
  SONAR_URL: "http://192.168.10.30:9000"

cache:
  paths:
    - .m2/repository
    - target/

build:
  stage: build
  image: maven:3.9-openjdk-17
  script:
    - mvn clean compile
  artifacts:
    paths:
      - target/
    expire_in: 1 hour

test:
  stage: test
  image: maven:3.9-openjdk-17
  script:
    - mvn test
  artifacts:
    when: always
    reports:
      junit:
        - target/surefire-reports/TEST-*.xml

sonarqube-check:
  stage: quality
  image: maven:3.9-openjdk-17
  variables:
    SONAR_USER_HOME: "${CI_PROJECT_DIR}/.sonar"
    GIT_DEPTH: "0"
  cache:
    key: "${CI_JOB_NAME}"
    paths:
      - .sonar/cache
  script:
    - mvn verify sonar:sonar
      -Dsonar.projectKey=${CI_PROJECT_NAME}
      -Dsonar.host.url=${SONAR_URL}
      -Dsonar.login=${SONAR_TOKEN}
  allow_failure: true

package:
  stage: package
  image: maven:3.9-openjdk-17
  script:
    - mvn package -DskipTests
    - mvn deploy:deploy-file
      -DgroupId=org.springframework.samples
      -DartifactId=spring-petclinic
      -Dversion=${CI_COMMIT_SHORT_SHA}
      -Dpackaging=jar
      -Dfile=target/spring-petclinic-*.jar
      -DrepositoryId=nexus
      -Durl=${NEXUS_URL}/repository/maven-releases/
  artifacts:
    paths:
      - target/*.jar
    expire_in: 1 week

build-docker:
  stage: package
  image: docker:24-dind
  services:
    - docker:24-dind
  variables:
    DOCKER_TLS_CERTDIR: "/certs"
  script:
    - docker build -t ${NEXUS_URL}:8082/petclinic:${CI_COMMIT_SHORT_SHA} .
    - docker login ${NEXUS_URL}:8082 -u admin -p ${NEXUS_PASSWORD}
    - docker push ${NEXUS_URL}:8082/petclinic:${CI_COMMIT_SHORT_SHA}

deploy-k8s:
  stage: deploy
  image: bitnami/kubectl:latest
  script:
```

Retry

A

Continue

bash

```bash
    - kubectl config set-cluster k3s --server=https://192.168.20.10:6443 --insecure-skip-tls-verify=true
    - kubectl config set-credentials gitlab-runner --token=${K8S_TOKEN}
    - kubectl config set-context default --cluster=k3s --user=gitlab-runner
    - kubectl config use-context default
    - |
      cat <<DEPLOY | kubectl apply -f -
      apiVersion: apps/v1
      kind: Deployment
      metadata:
        name: petclinic
        namespace: default
      spec:
        replicas: 3
        selector:
          matchLabels:
            app: petclinic
        template:
          metadata:
            labels:
              app: petclinic
          spec:
            containers:
            - name: petclinic
              image: ${NEXUS_URL}:8082/petclinic:${CI_COMMIT_SHORT_SHA}
              ports:
              - containerPort: 8080
              resources:
                requests:
                  memory: "512Mi"
                  cpu: "500m"
                limits:
                  memory: "1Gi"
                  cpu: "1000m"
              livenessProbe:
                httpGet:
                  path: /actuator/health
                  port: 8080
                initialDelaySeconds: 60
                periodSeconds: 10
              readinessProbe:
                httpGet:
                  path: /actuator/health
                  port: 8080
                initialDelaySeconds: 30
                periodSeconds: 5
      ---
      apiVersion: v1
      kind: Service
      metadata:
        name: petclinic-service
        namespace: default
      spec:
        selector:
          app: petclinic
        ports:
        - protocol: TCP
          port: 80
          targetPort: 8080
        type: LoadBalancer
      ---
      apiVersion: networking.k8s.io/v1
      kind: Ingress
      metadata:
        name: petclinic-ingress
        namespace: default
        annotations:
          nginx.ingress.kubernetes.io/rewrite-target: /
      spec:
        ingressClassName: nginx
        rules:
        - host: petclinic.yourdomain.com
          http:
            paths:
            - path: /
              pathType: Prefix
              backend:
                service:
                  name: petclinic-service
                  port:
                    number: 80
      DEPLOY
  only:
    - main
EOF

# Создаем Dockerfile для PetClinic
cat << 'EOF' > Dockerfile
FROM openjdk:17-jdk-slim
WORKDIR /app
COPY target/spring-petclinic-*.jar app.jar
EXPOSE 8080
ENTRYPOINT ["java", "-jar", "app.jar"]
EOF

# Коммитим и пушим
git add .
git commit -m "Add CI/CD pipeline configuration"
git push -u origin main
```

### 5.3 Настройка Nexus для Docker Registry

bash

```bash
# Через Nexus UI (http://192.168.30.1:8081)
# 1. Login as admin
# 2. Create Docker (hosted) repository:
#    - Name: docker-hosted
#    - HTTP port: 8082
#    - Enable Docker V1 API: false
#    - Deployment policy: Allow redeploy

# На всех K8s worker нодах добавляем insecure registry
cat << EOF > /etc/rancher/k3s/registries.yaml
mirrors:
  "192.168.10.20:8082":
    endpoint:
      - "http://192.168.10.20:8082"
configs:
  "192.168.10.20:8082":
    auth:
      username: admin
      password: your-nexus-password
EOF

# Перезапуск K3s на workers
systemctl restart k3s-agent

# Создаем Kubernetes secret для Docker registry
kubectl create secret docker-registry nexus-registry \
  --docker-server=192.168.10.20:8082 \
  --docker-username=admin \
  --docker-password=your-nexus-password \
  --namespace=default

# Добавляем imagePullSecrets в deployment (уже включено в .gitlab-ci.yml выше)
```

### 5.4 Настройка SonarQube Token

bash

```bash
# В SonarQube UI (http://192.168.30.1:9000)
# 1. Login as admin
# 2. My Account -> Security -> Generate Token
# 3. Copy token

# В GitLab UI
# Settings -> CI/CD -> Variables
# Add variable:
#   Key: SONAR_TOKEN
#   Value: <your-sonarqube-token>
#   Protected: Yes
#   Masked: Yes
```

---

## Этап 6: Мониторинг и Observability

### 6.1 Установка Prometheus Stack

bash

```bash
# Добавляем Helm repo
helm repo add prometheus-community https://prometheus-community.github.io/helm-charts
helm repo update

# Создаем namespace
kubectl create namespace monitoring

# Создаем values.yaml для кастомизации
cat << 'EOF' > prometheus-values.yaml
prometheus:
  prometheusSpec:
    retention: 15d
    storageSpec:
      volumeClaimTemplate:
        spec:
          storageClassName: longhorn
          accessModes: ["ReadWriteOnce"]
          resources:
            requests:
              storage: 50Gi
    resources:
      requests:
        cpu: 500m
        memory: 2Gi
      limits:
        cpu: 2000m
        memory: 4Gi

grafana:
  enabled: true
  adminPassword: "StrongPassword123"
  persistence:
    enabled: true
    storageClassName: longhorn
    size: 10Gi
  ingress:
    enabled: true
    ingressClassName: nginx
    hosts:
      - grafana.yourdomain.com

alertmanager:
  enabled: true
  alertmanagerSpec:
    storage:
      volumeClaimTemplate:
        spec:
          storageClassName: longhorn
          accessModes: ["ReadWriteOnce"]
          resources:
            requests:
              storage: 10Gi

kubeStateMetrics:
  enabled: true

nodeExporter:
  enabled: true

prometheusOperator:
  enabled: true
EOF

# Установка
helm install prometheus prometheus-community/kube-prometheus-stack \
  --namespace monitoring \
  -f prometheus-values.yaml

# Проверка
kubectl get pods -n monitoring -w
```

### 6.2 Настройка ServiceMonitor для приложений

bash

```bash
# Создаем ServiceMonitor для PetClinic
cat << EOF | kubectl apply -f -
apiVersion: v1
kind: Service
metadata:
  name: petclinic-metrics
  namespace: default
  labels:
    app: petclinic
spec:
  selector:
    app: petclinic
  ports:
  - name: metrics
    port: 8080
    targetPort: 8080
---
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: petclinic-monitor
  namespace: monitoring
  labels:
    release: prometheus
spec:
  selector:
    matchLabels:
      app: petclinic
  namespaceSelector:
    matchNames:
    - default
  endpoints:
  - port: metrics
    path: /actuator/prometheus
    interval: 30s
EOF
```

### 6.3 Мониторинг Infrastructure Services

bash

```bash
# На GitLab VM устанавливаем node_exporter
cd /tmp
wget https://github.com/prometheus/node_exporter/releases/download/v1.7.0/node_exporter-1.7.0.linux-amd64.tar.gz
tar xvfz node_exporter-1.7.0.linux-amd64.tar.gz
cp node_exporter-1.7.0.linux-amd64/node_exporter /usr/local/bin/

cat << 'EOF' > /etc/systemd/system/node_exporter.service
[Unit]
Description=Node Exporter
After=network.target

[Service]
Type=simple
ExecStart=/usr/local/bin/node_exporter
Restart=on-failure

[Install]
WantedBy=multi-user.target
EOF

systemctl daemon-reload
systemctl enable --now node_exporter

# Повторить для Nexus, SonarQube, PostgreSQL, HAProxy VMs

# На Prometheus добавляем scrape configs
kubectl edit configmap prometheus-prometheus-kube-prometheus-prometheus -n monitoring

# Добавляем в scrape_configs:
# - job_name: 'infrastructure'
#   static_configs:
#   - targets:
#     - '192.168.10.10:9100'  # GitLab
#     - '192.168.10.20:9100'  # Nexus
#     - '192.168.10.30:9100'  # SonarQube
#     - '192.168.10.40:9100'  # PostgreSQL
#     - '192.168.30.10:9100'  # HAProxy-1
#     - '192.168.30.11:9100'  # HAProxy-2
```

### 6.4 Установка Loki для логирования

bash

```bash
# Добавляем Helm repo
helm repo add grafana https://grafana.github.io/helm-charts
helm repo update

# Создаем values для Loki
cat << 'EOF' > loki-values.yaml
loki:
  auth_enabled: false
  commonConfig:
    replication_factor: 1
  storage:
    type: filesystem
  schemaConfig:
    configs:
    - from: 2024-01-01
      store: tsdb
      object_store: filesystem
      schema: v13
      index:
        prefix: index_
        period: 24h

singleBinary:
  replicas: 1
  persistence:
    enabled: true
    storageClass: longhorn
    size: 50Gi

write:
  replicas: 0

read:
  replicas: 0

backend:
  replicas: 0

monitoring:
  selfMonitoring:
    enabled: false
  lokiCanary:
    enabled: false

test:
  enabled: false
EOF

# Установка Loki
helm install loki grafana/loki \
  --namespace monitoring \
  -f loki-values.yaml

# Установка Promtail для сбора логов
cat << 'EOF' > promtail-values.yaml
config:
  clients:
    - url: http://loki:3100/loki/api/v1/push
  snippets:
    scrapeConfigs: |
      - job_name: kubernetes-pods
        kubernetes_sd_configs:
          - role: pod
        relabel_configs:
          - source_labels: [__meta_kubernetes_pod_node_name]
            target_label: __host__
          - action: labelmap
            regex: __meta_kubernetes_pod_label_(.+)
          - action: replace
            replacement: $1
            separator: /
            source_labels:
            - __meta_kubernetes_namespace
            - __meta_kubernetes_pod_name
            target_label: job
EOF

helm install promtail grafana/promtail \
  --namespace monitoring \
  -f promtail-values.yaml

# Добавляем Loki data source в Grafana
kubectl exec -it -n monitoring deployment/prometheus-grafana -- grafana-cli plugins install grafana-loki-datasource
```

---

## Этап 7: Backup и Disaster Recovery

### 7.1 Backup Strategy для GitLab

bash

```bash
# На GitLab VM создаем скрипт бэкапа
cat << 'EOF' > /usr/local/bin/gitlab-backup.sh
#!/bin/bash
set -e

BACKUP_DIR="/var/opt/gitlab/backups"
RETENTION_DAYS=7
NFS_BACKUP="/mnt/nfs-backup"

# Создаем backup
gitlab-backup create SKIP=registry

# Копируем на NFS
rsync -av ${BACKUP_DIR}/ ${NFS_BACKUP}/gitlab/

# Backup конфигурации
tar -czf ${NFS_BACKUP}/gitlab/gitlab-config-$(date +%Y%m%d).tar.gz \
  /etc/gitlab/gitlab.rb \
  /etc/gitlab/gitlab-secrets.json

# Очистка старых бэкапов
find ${BACKUP_DIR} -name "*.tar" -mtime +${RETENTION_DAYS} -delete
find ${NFS_BACKUP}/gitlab -name "*.tar" -mtime +${RETENTION_DAYS} -delete

echo "Backup completed: $(date)"
EOF

chmod +x /usr/local/bin/gitlab-backup.sh

# Добавляем в cron (ежедневно в 2 AM)
cat << 'EOF' > /etc/cron.d/gitlab-backup
0 2 * * * root /usr/local/bin/gitlab-backup.sh >> /var/log/gitlab-backup.log 2>&1
EOF
```

### 7.2 Backup для Nexus

bash

```bash
# На Nexus VM
cat << 'EOF' > /usr/local/bin/nexus-backup.sh
#!/bin/bash
set -e

NEXUS_DATA="/opt/sonatype-work/nexus3"
BACKUP_DIR="/mnt/nfs-backup/nexus"
DATE=$(date +%Y%m%d_%H%M%S)

# Создаем backup через Nexus API
curl -u admin:${NEXUS_PASSWORD} -X POST \
  "http://localhost:8081/service/rest/v1/tasks/run/db.backup"

# Ждем завершения задачи
sleep 300

# Копируем бэкап на NFS
rsync -av ${NEXUS_DATA}/backup/ ${BACKUP_DIR}/backup-${DATE}/

# Очистка старых бэкапов (7 дней)
find ${BACKUP_DIR} -maxdepth 1 -type d -mtime +7 -exec rm -rf {} \;

echo "Nexus backup completed: $(date)"
EOF

chmod +x /usr/local/bin/nexus-backup.sh

# Cron job (ежедневно в 3 AM)
cat << 'EOF' > /etc/cron.d/nexus-backup
0 3 * * * root /usr/local/bin/nexus-backup.sh >> /var/log/nexus-backup.log 2>&1
EOF
```

### 7.3 Backup PostgreSQL

bash

```bash
# На PostgreSQL VM
cat << 'EOF' > /usr/local/bin/postgres-backup.sh
#!/bin/bash
set -e

BACKUP_DIR="/mnt/nfs-backup/postgresql"
DATE=$(date +%Y%m%d_%H%M%S)

# Backup всех баз
sudo -u postgres pg_dumpall > ${BACKUP_DIR}/all-databases-${DATE}.sql

# Backup отдельных баз
sudo -u postgres pg_dump gitlab_production > ${BACKUP_DIR}/gitlab-${DATE}.sql
sudo -u postgres pg_dump sonarqube > ${BACKUP_DIR}/sonarqube-${DATE}.sql

# Сжатие
gzip ${BACKUP_DIR}/*-${DATE}.sql

# Очистка старых бэкапов
find ${BACKUP_DIR} -name "*.sql.gz" -mtime +7 -delete

echo "PostgreSQL backup completed: $(date)"
EOF

chmod +x /usr/local/bin/postgres-backup.sh

# Cron job
cat << 'EOF' > /etc/cron.d/postgres-backup
0 1 * * * root /usr/local/bin/postgres-backup.sh >> /var/log/postgres-backup.log 2>&1
EOF
```

### 7.4 Kubernetes Backup с Velero

bash

```bash
# На master-1
# Установка Velero CLI
wget https://github.com/vmware-tanzu/velero/releases/download/v1.12.1/velero-v1.12.1-linux-amd64.tar.gz
tar -xvf velero-v1.12.1-linux-amd64.tar.gz
mv velero-v1.12.1-linux-amd64/velero /usr/local/bin/

# Настройка NFS backend для Velero
cat << 'EOF' > velero-values.yaml
configuration:
  backupStorageLocation:
    - name: default
      provider: aws
      bucket: k8s-backups
      config:
        region: minio
        s3ForcePathStyle: true
        s3Url: http://192.168.10.50:9000
  volumeSnapshotLocation:
    - name: default
      provider: aws
      config:
        region: minio

credentials:
  useSecret: true
  secretContents:
    cloud: |
      [default]
      aws_access_key_id = minioadmin
      aws_secret_access_key = minioadmin

initContainers:
  - name: velero-plugin-for-aws
    image: velero/velero-plugin-for-aws:v1.8.0
    volumeMounts:
      - mountPath: /target
        name: plugins

schedules:
  daily-backup:
    schedule: "0 2 * * *"
    template:
      ttl: "168h"
      includedNamespaces:
      - "*"
EOF

# Установка MinIO на NFS сервере (192.168.10.50)
wget https://dl.min.io/server/minio/release/linux-amd64/minio
chmod +x minio
mv minio /usr/local/bin/

cat << 'EOF' > /etc/systemd/system/minio.service
[Unit]
Description=MinIO
After=network.target

[Service]
Type=simple
ExecStart=/usr/local/bin/minio server /export/minio-data --console-address ":9001"
Restart=on-failure
Environment="MINIO_ROOT_USER=minioadmin"
Environment="MINIO_ROOT_PASSWORD=minioadmin"

[Install]
WantedBy=multi-user.target
EOF

systemctl daemon-reload
systemctl enable --now minio

# На K8s master
helm repo add vmware-tanzu https://vmware-tanzu.github.io/helm-charts
helm install velero vmware-tanzu/velero \
  --namespace velero \
  --create-namespace \
  -f velero-values.yaml

# Тестовый бэкап
velero backup create test-backup --include-namespaces default
velero backup describe test-backup
```

---

## Этап 8: Security Hardening

### 8.1 Настройка SSL/TLS сертификатов

bash

```bash
# Установка cert-manager в K8s
kubectl apply -f https://github.com/cert-manager/cert-manager/releases/download/v1.13.3/cert-manager.yaml

# Ждем готовности
kubectl wait --for=condition=Available --timeout=300s \
  deployment/cert-manager -n cert-manager

# Создаем ClusterIssuer для Let's Encrypt (если есть публичный домен)
cat << EOF | kubectl apply -f -
apiVersion: cert-manager.io/v1
kind: ClusterIssuer
metadata:
  name: letsencrypt-prod
spec:
  acme:
    server: https://acme-v02.api.letsencrypt.org/directory
    email: admin@yourdomain.com
    privateKeySecretRef:
      name: letsencrypt-prod
    solvers:
    - http01:
        ingress:
          class: nginx
EOF

# Для внутренней сети создаем self-signed issuer
cat << EOF | kubectl apply -f -
apiVersion: cert-manager.io/v1
kind: ClusterIssuer
metadata:
  name: selfsigned-issuer
spec:
  selfSigned: {}
EOF

# Обновляем Ingress для использования TLS
cat << EOF | kubectl apply -f -
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: petclinic-ingress
  namespace: default
  annotations:
    cert-manager.io/cluster-issuer: "selfsigned-issuer"
    nginx.ingress.kubernetes.io/ssl-redirect: "true"
spec:
  ingressClassName: nginx
  tls:
  - hosts:
    - petclinic.yourdomain.com
    secretName: petclinic-tls
  rules:
  - host: petclinic.yourdomain.com
    http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: petclinic-service
            port:
              number: 80
EOF
```

### 8.2 Network Policies

bash

```bash
# Базовая Network Policy для изоляции namespaces
cat << EOF | kubectl apply -f -
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: deny-all-ingress
  namespace: default
spec:
  podSelector: {}
  policyTypes:
  - Ingress
---
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: allow-ingress-nginx
  namespace: default
spec:
  podSelector:
    matchLabels:
      app: petclinic
  policyTypes:
  - Ingress
  ingress:
  - from:
    - namespaceSelector:
        matchLabels:
          name: ingress-nginx
    ports:
    - protocol: TCP
      port: 8080
---
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: allow-monitoring
  namespace: default
spec:
  podSelector: {}
  policyTypes:
  - Ingress
  ingress:
  - from:
    - namespaceSelector:
        matchLabels:
          name: monitoring
    ports:
    - protocol: TCP
      port: 8080
EOF
```

### 8.3 Pod Security Standards

bash

```bash
# Включаем Pod Security Admission
cat << EOF | kubectl apply -f -
apiVersion: v1
kind: Namespace
metadata:
  name: production
  labels:
    pod-security.kubernetes.io/enforce: restricted
    pod-security.kubernetes.io/audit: restricted
    pod-security.kubernetes.io/warn: restricted
EOF

# Создаем SecurityContext для приложений
cat << EOF > petclinic-secure-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: petclinic-secure
  namespace: production
spec:
  replicas: 3
  selector:
    matchLabels:
      app: petclinic
  template:
    metadata:
      labels:
        app: petclinic
    spec:
      securityContext:
        runAsNonRoot: true
        runAsUser: 1000
        fsGroup: 1000
        seccompProfile:
          type: RuntimeDefault
      containers:
      - name: petclinic
        image: 192.168.10.20:8082/petclinic:latest
        securityContext:
          allowPrivilegeEscalation: false
          capabilities:
            drop:
            - ALL
          readOnlyRootFilesystem: true
        ports:
        - containerPort: 8080
        volumeMounts:
        - name: tmp
          mountPath: /tmp
        - name: cache
          mountPath: /app/cache
        resources:
          requests:
            memory: "512Mi"
            cpu: "500m"
          limits:
            memory: "1Gi"
            cpu: "1000m"
      volumes:
      - name: tmp
        emptyDir: {}
      - name: cache
        emptyDir: {}
EOF

kubectl apply -f petclinic-secure-deployment.yaml
```

### 8.4 Secrets Management

bash

```bash
# Установка Sealed Secrets для безопасного хранения секретов в Git
kubectl apply -f https://github.com/bitnami-labs/sealed-secrets/releases/download/v0.24.5/controller.yaml

# Установка kubeseal CLI
wget https://github.com/bitnami-labs/sealed-secrets/releases/download/v0.24.5/kubeseal-0.24.5-linux-amd64.tar.gz
tar -xvzf kubeseal-0.24.5-linux-amd64.tar.gz
mv kubeseal /usr/local/bin/

# Пример создания sealed secret
kubectl create secret generic db-credentials \
  --from-literal=username=appuser \
  --from-literal=password=SecurePass123 \
  --dry-run=client -o yaml | \
  kubeseal -o yaml > sealed-db-credentials.yaml

# Теперь sealed-db-credentials.yaml можно безопасно commit в Git
kubectl apply -f sealed-db-credentials.yaml
```

---

## Этап 9: Тестирование и Валидация

### 9.1 Smoke Tests

bash

```bash
# Создаем скрипт для проверки всех компонентов
cat << 'EOF' > /usr/local/bin/infrastructure-health-check.sh
#!/bin/bash

echo "=== Infrastructure Health Check ==="
echo "Date: $(date)"
echo ""

# Check HAProxy VIP
echo "Checking HAProxy VIP..."
ping -c 2 192.168.30.1 && echo "✓ VIP accessible" || echo "✗ VIP failed"
echo ""

# Check GitLab
echo "Checking GitLab..."
curl -s -o /dev/null -w "%{http_code}" http://192.168.30.1 | \
  grep -q "302\|200" && echo "✓ GitLab accessible" || echo "✗ GitLab failed"
echo ""

# Check Nexus
echo "Checking Nexus..."
curl -s -o /dev/null -w "%{http_code}" http://192.168.30.1:8081 | \
  grep -q "200" && echo "✓ Nexus accessible" || echo "✗ Nexus failed"
echo ""

# Check SonarQube
echo "Checking SonarQube..."
curl -s -o /dev/null -w "%{http_code}" http://192.168.30.1:9000 | \
  grep -q "200" && echo "✓ SonarQube accessible" || echo "✗ SonarQube failed"
echo ""

# Check Kubernetes API
echo "Checking Kubernetes..."
kubectl cluster-info && echo "✓ K8s API accessible" || echo "✗ K8s API failed"
echo ""

# Check K8s nodes
echo "Kubernetes Nodes:"
kubectl get nodes
echo ""

# Check K8s pods
echo "Critical Pods Status:"
kubectl get pods -n kube-system
kubectl get pods -n monitoring
kubectl get pods -n ingress-nginx
echo ""

# Check PetClinic deployment
echo "Checking PetClinic Application..."
kubectl get deployment petclinic -n default && \
  echo "✓ PetClinic deployed" || echo "✗ PetClinic not found"
echo ""

# Check MetalLB
echo "Checking LoadBalancer Services..."
kubectl get svc -A | grep LoadBalancer
echo ""

# Check storage
echo "Checking Persistent Volumes..."
kubectl get pv
echo ""

echo "=== Health Check Complete ==="
EOF

chmod +x /usr/local/bin/infrastructure-health-check.sh

# Запускаем проверку
/usr/local/bin/infrastructure-health-check.sh
```

### 9.2 Load Testing

bash

```bash
# Установка k6 для load testing
apt install -y gpg
gpg --no-default-keyring --keyring /usr/share/keyrings/k6-archive-keyring.gpg \
  --keyserver hkp://keyserver.ubuntu.com:80 --recv-keys C5AD17C747E3415A3642D57D77C6C491D6AC1D69
echo "deb [signed-by=/usr/share/keyrings/k6-archive-keyring.gpg] https://dl.k6.io/deb stable main" | \
  tee /etc/apt/sources.list.d/k6.list
apt update && apt install -y k6

# Создаем load test script
cat << 'EOF' > petclinic-load-test.js
import http from 'k6/http';
import { check, sleep } from 'k6';

export const options = {
  stages: [
    { duration: '2m', target: 50 },  // Ramp up to 50 users
    { duration: '5m', target: 50 },  // Stay at 50 users
    { duration: '2m', target: 100 }, // Ramp up to 100 users
    { duration: '5m', target: 100 }, // Stay at 100 users
    { duration: '2m', target: 0 },   // Ramp down to 0 users
  ],
  thresholds: {
    http_req_duration: ['p(95)<500'], // 95% of requests should be below 500ms
    http_req_failed: ['rate<0.01'],   // Error rate should be less than 1%
  },
};

export default function () {
  const res = http.get('http://petclinic.yourdomain.com');
  
  check(res, {
    'status is 200': (r) => r.status === 200,
    'response time < 500ms': (r) => r.timings.duration < 500,
  });
  
  sleep(1);
}
EOF

# Запуск load test
k6 run petclinic-load-test.js
```

### 9.3 Failover Testing

bash

```bash
# Тестируем HAProxy failover
cat << 'EOF' > /usr/local/bin/test-haproxy-failover.sh
#!/bin/bash

echo "Testing HAProxy Failover..."

# Проверяем текущий master
echo "Current VRRP Master:"
ssh 192.168.30.10 "ip addr show eth0 | grep 192.168.30.1" && \
  echo "HAProxy-1 is MASTER" || echo "HAProxy-1 is BACKUP"
ssh 192.168.30.11 "ip addr show eth0 | grep 192.168.30.1" && \
  echo "HAProxy-2 is MASTER" || echo "HAProxy-2 is BACKUP"

echo ""
echo "Stopping HAProxy on current master..."
ssh 192.168.30.10 "systemctl stop haproxy"

echo "Waiting 10 seconds for failover..."
sleep 10

echo "New VRRP Master:"
ssh 192.168.30.10 "ip addr show eth0 | grep 192.168.30.1" && \
  echo "HAProxy-1 is MASTER" || echo "HAProxy-1 is BACKUP"
ssh 192.168.30.11 "ip addr show eth0 | grep 192.168.30.1" && \
  echo "HAProxy-2 is MASTER" || echo "HAProxy-2 is BACKUP"

echo ""
echo "Testing VIP accessibility..."
curl -s -o /dev/null -w "%{http_code}" http://192.168.30.1 && \
  echo "✓ VIP still accessible after failover"

echo ""
echo "Restoring HAProxy on original master..."
ssh 192.168.30.10 "systemctl start haproxy"

echo "Failover test complete
```

Retry

A

Continue

bash

```bash
echo "Failover test complete"
EOF

chmod +x /usr/local/bin/test-haproxy-failover.sh

# Тестируем K8s master failover
cat << 'EOF' > /usr/local/bin/test-k8s-failover.sh
#!/bin/bash

echo "Testing K8s Control Plane Failover..."

# Проверяем доступность всех masters
echo "Checking all master nodes..."
for i in 10 11 12; do
  echo "Testing k3s-master at 192.168.20.$i"
  curl -sk https://192.168.20.$i:6443/livez && \
    echo "✓ Master $i is healthy" || echo "✗ Master $i is down"
done

echo ""
echo "Current leader:"
kubectl get lease -n kube-system kube-controller-manager -o jsonpath='{.spec.holderIdentity}'
echo ""

# Останавливаем первый master
echo "Stopping k3s-master-1..."
ssh 192.168.20.10 "systemctl stop k3s"

echo "Waiting 30 seconds for failover..."
sleep 30

echo "Testing cluster accessibility..."
kubectl get nodes && echo "✓ Cluster still accessible" || echo "✗ Cluster unreachable"

echo ""
echo "New leader:"
kubectl get lease -n kube-system kube-controller-manager -o jsonpath='{.spec.holderIdentity}'
echo ""

echo "Restoring k3s-master-1..."
ssh 192.168.20.10 "systemctl start k3s"

sleep 20
kubectl get nodes

echo "K8s failover test complete"
EOF

chmod +x /usr/local/bin/test-k8s-failover.sh
```

---

## Этап 10: Documentation и Runbooks

### 10.1 Создание документации инфраструктуры

bash

````bash
# Создаем директорию для документации в GitLab
mkdir -p /tmp/infrastructure-docs
cd /tmp/infrastructure-docs

cat << 'EOF' > README.md
# Production DevOps Infrastructure Documentation

## Architecture Overview

### Network Topology
- **Management VLAN (10.0.10.0/24)**: Bastion host, administrative access
- **Infrastructure VLAN (192.168.10.0/24)**: GitLab, Nexus, SonarQube, PostgreSQL
- **Kubernetes VLAN (192.168.20.0/24)**: K3s cluster (3 masters, 3 workers)
- **DMZ VLAN (192.168.30.0/24)**: HAProxy load balancers with VRRP

### Services Inventory

| Service | IP Address | Version | Purpose |
|---------|------------|---------|---------|
| HAProxy VIP | 192.168.30.1 | - | Virtual IP for load balancing |
| HAProxy-1 | 192.168.30.10 | 2.8+ | Load balancer (VRRP Master) |
| HAProxy-2 | 192.168.30.11 | 2.8+ | Load balancer (VRRP Backup) |
| GitLab Primary | 192.168.10.10 | 16+ | Source control & CI/CD |
| GitLab Secondary | 192.168.10.11 | 16+ | HA backup |
| Nexus | 192.168.10.20 | 3.x | Artifact repository |
| SonarQube | 192.168.10.30 | 10.x | Code quality analysis |
| PostgreSQL | 192.168.10.40 | 15 | Shared database |
| NFS Server | 192.168.10.50 | - | Shared storage |
| K3s Master-1 | 192.168.20.10 | v1.28+ | Kubernetes control plane |
| K3s Master-2 | 192.168.20.11 | v1.28+ | Kubernetes control plane |
| K3s Master-3 | 192.168.20.12 | v1.28+ | Kubernetes control plane |
| K3s Worker-1 | 192.168.20.21 | v1.28+ | Kubernetes worker node |
| K3s Worker-2 | 192.168.20.22 | v1.28+ | Kubernetes worker node |
| K3s Worker-3 | 192.168.20.23 | v1.28+ | Kubernetes worker node |

### Access Points

| Service | URL | Credentials |
|---------|-----|-------------|
| GitLab | https://gitlab.yourdomain.com | root / (see /etc/gitlab/initial_root_password) |
| Nexus | http://192.168.30.1:8081 | admin / (see /opt/sonatype-work/nexus3/admin.password) |
| SonarQube | http://192.168.30.1:9000 | admin / admin |
| Grafana | http://grafana.yourdomain.com | admin / StrongPassword123 |
| HAProxy Stats | http://192.168.30.1:8404/stats | - |
| Kubernetes API | https://192.168.20.10:6443 | kubectl config |

## Daily Operations

### Service Status Checks
```bash
# Run comprehensive health check
/usr/local/bin/infrastructure-health-check.sh

# Check individual services
systemctl status gitlab-runsvdir
systemctl status nexus
systemctl status sonarqube
systemctl status postgresql
systemctl status haproxy
systemctl status keepalived

# Kubernetes status
kubectl get nodes
kubectl get pods -A
kubectl top nodes
kubectl top pods -A
```

### Monitoring
- **Grafana Dashboards**: http://grafana.yourdomain.com
  - Node Exporter Full
  - Kubernetes Cluster Monitoring
  - Longhorn Dashboard
  - HAProxy Statistics

- **Prometheus Alerts**: Check Alertmanager for active alerts
- **Loki Logs**: Query logs through Grafana Explore

### Backup Verification
```bash
# Check backup jobs
cat /var/log/gitlab-backup.log
cat /var/log/nexus-backup.log
cat /var/log/postgres-backup.log

# Verify backup storage
ls -lh /mnt/nfs-backup/gitlab/
ls -lh /mnt/nfs-backup/nexus/
ls -lh /mnt/nfs-backup/postgresql/

# Check Velero backups
velero backup get
velero backup describe <backup-name>
```

## Troubleshooting

### GitLab Issues
```bash
# Check GitLab status
gitlab-ctl status

# View logs
gitlab-ctl tail

# Reconfigure if needed
gitlab-ctl reconfigure

# Check database connection
gitlab-rake gitlab:check

# Restart services
gitlab-ctl restart
```

### Kubernetes Issues
```bash
# Check node status
kubectl describe node <node-name>

# Check pod logs
kubectl logs -f <pod-name> -n <namespace>

# Check events
kubectl get events -A --sort-by='.lastTimestamp'

# Restart failed pods
kubectl delete pod <pod-name> -n <namespace>

# Check etcd health (on masters)
kubectl exec -n kube-system etcd-k3s-master-1 -- \
  etcdctl --cacert=/var/lib/rancher/k3s/server/tls/etcd/server-ca.crt \
  --cert=/var/lib/rancher/k3s/server/tls/etcd/server-client.crt \
  --key=/var/lib/rancher/k3s/server/tls/etcd/server-client.key \
  endpoint health
```

### Storage Issues
```bash
# Check NFS mounts
df -h | grep nfs
showmount -e 192.168.10.50

# Check Longhorn volumes
kubectl get pv
kubectl get pvc -A
kubectl -n longhorn-system get volumes

# Remount NFS if needed
umount /var/opt/gitlab
mount -a
```

### Network Issues
```bash
# Check VRRP status
ip addr show | grep 192.168.30.1

# Test connectivity between VLANs
ping 192.168.10.10  # Infrastructure
ping 192.168.20.10  # Kubernetes
ping 192.168.30.1   # DMZ VIP

# Check HAProxy backend health
echo "show stat" | socat stdio /run/haproxy/admin.sock
```

## Disaster Recovery Procedures

### GitLab Recovery
```bash
# Stop GitLab
gitlab-ctl stop unicorn
gitlab-ctl stop puma
gitlab-ctl stop sidekiq

# Restore from backup
cd /var/opt/gitlab/backups
gitlab-backup restore BACKUP=<timestamp>

# Restore configuration
tar -xzf /mnt/nfs-backup/gitlab/gitlab-config-YYYYMMDD.tar.gz -C /

# Reconfigure and restart
gitlab-ctl reconfigure
gitlab-ctl restart
gitlab-rake gitlab:check SANITIZE=true
```

### PostgreSQL Recovery
```bash
# Stop applications using PostgreSQL
systemctl stop gitlab-runsvdir sonarqube

# Restore database
sudo -u postgres psql < /mnt/nfs-backup/postgresql/all-databases-YYYYMMDD.sql.gz

# Or restore individual databases
gunzip -c /mnt/nfs-backup/postgresql/gitlab-YYYYMMDD.sql.gz | \
  sudo -u postgres psql gitlab_production

# Restart services
systemctl start gitlab-runsvdir sonarqube
```

### Kubernetes Recovery with Velero
```bash
# List available backups
velero backup get

# Restore entire cluster
velero restore create --from-backup <backup-name>

# Restore specific namespace
velero restore create --from-backup <backup-name> \
  --include-namespaces default

# Monitor restore progress
velero restore describe <restore-name>
velero restore logs <restore-name>
```

### Complete Infrastructure Rebuild
See DISASTER_RECOVERY.md for complete rebuild procedures.

## Security Procedures

### Certificate Renewal
```bash
# Let's Encrypt certificates (if using)
certbot renew --dry-run

# Check certificate expiration
echo | openssl s_client -servername gitlab.yourdomain.com \
  -connect gitlab.yourdomain.com:443 2>/dev/null | \
  openssl x509 -noout -dates

# Kubernetes certificates
kubeadm certs check-expiration
```

### Password Rotation
```bash
# GitLab root password
gitlab-rake "gitlab:password:reset[root]"

# PostgreSQL passwords
sudo -u postgres psql
ALTER USER gitlab WITH PASSWORD 'new_password';
ALTER USER sonarqube WITH PASSWORD 'new_password';

# Update application configurations accordingly
```

### Security Scanning
```bash
# Scan containers for vulnerabilities
trivy image 192.168.10.20:8082/petclinic:latest

# Check for outdated packages
kubectl get pods -A -o jsonpath='{range .items[*]}{.metadata.name}{"\t"}{.spec.containers[*].image}{"\n"}{end}'

# Run CIS benchmark
kube-bench run --targets master,node
```

## Maintenance Windows

### Monthly Maintenance Tasks
- [ ] Update all system packages
- [ ] Review and rotate logs
- [ ] Verify backup integrity
- [ ] Check disk space usage
- [ ] Review security advisories
- [ ] Test disaster recovery procedures
- [ ] Update documentation

### Quarterly Maintenance Tasks
- [ ] Upgrade GitLab
- [ ] Upgrade Kubernetes
- [ ] Review and update SSL certificates
- [ ] Capacity planning review
- [ ] Security audit
- [ ] Update runbooks

## Contact Information

**Infrastructure Team**
- Email: devops@yourdomain.com
- Slack: #infrastructure
- PagerDuty: https://yourdomain.pagerduty.com

**Escalation Path**
1. On-call DevOps Engineer
2. Lead DevOps Engineer
3. Infrastructure Manager
4. CTO

## Additional Resources
- [HAProxy Configuration](./haproxy/README.md)
- [Kubernetes Operations](./kubernetes/README.md)
- [GitLab Administration](./gitlab/README.md)
- [Monitoring Setup](./monitoring/README.md)
- [Disaster Recovery Plan](./DISASTER_RECOVERY.md)
EOF

# Создаем детальный runbook для common tasks
cat << 'EOF' > RUNBOOK.md
# Infrastructure Runbook

## Table of Contents
1. [Service Restarts](#service-restarts)
2. [Scaling Operations](#scaling-operations)
3. [Backup and Restore](#backup-and-restore)
4. [Performance Tuning](#performance-tuning)
5. [Emergency Procedures](#emergency-procedures)

## Service Restarts

### Restart GitLab
```bash
# Graceful restart
gitlab-ctl restart

# Restart specific component
gitlab-ctl restart nginx
gitlab-ctl restart postgresql
gitlab-ctl restart redis

# Force restart if hung
gitlab-ctl kill <service>
gitlab-ctl start <service>
```

### Restart Nexus
```bash
# Graceful restart
systemctl restart nexus

# Check status
systemctl status nexus
tail -f /opt/sonatype-work/nexus3/log/nexus.log

# If unresponsive, force restart
systemctl stop nexus
pkill -9 -f nexus
systemctl start nexus
```

### Restart SonarQube
```bash
# Graceful restart
systemctl restart sonarqube

# Check logs
tail -f /opt/sonarqube/logs/sonar.log
tail -f /opt/sonarqube/logs/web.log

# Force restart if needed
systemctl stop sonarqube
pkill -9 -f sonarqube
systemctl start sonarqube
```

### Restart PostgreSQL
```bash
# WARNING: This will interrupt all dependent services
# Stop applications first
systemctl stop gitlab-runsvdir sonarqube

# Restart PostgreSQL
systemctl restart postgresql

# Restart applications
systemctl start gitlab-runsvdir sonarqube

# Verify connections
sudo -u postgres psql -c "SELECT * FROM pg_stat_activity;"
```

### Restart HAProxy
```bash
# On both HAProxy nodes
systemctl reload haproxy  # Graceful reload (preferred)
systemctl restart haproxy  # Full restart

# Verify VRRP status
ip addr show eth0 | grep 192.168.30.1
systemctl status keepalived
```

### Rolling Restart of K8s Workers
```bash
# Drain node
kubectl drain k3s-worker-1 --ignore-daemonsets --delete-emptydir-data

# Restart node
ssh 192.168.20.21 "systemctl restart k3s-agent"

# Uncordon node
kubectl uncordon k3s-worker-1

# Wait for node ready
kubectl wait --for=condition=Ready node/k3s-worker-1 --timeout=300s

# Repeat for other workers
```

## Scaling Operations

### Scale Application Deployments
```bash
# Scale up
kubectl scale deployment petclinic --replicas=5 -n default

# Scale down
kubectl scale deployment petclinic --replicas=2 -n default

# Autoscaling
kubectl autoscale deployment petclinic \
  --cpu-percent=70 \
  --min=3 \
  --max=10 \
  -n default

# Check HPA status
kubectl get hpa -n default
```

### Add Kubernetes Worker Node
```bash
# On new worker node (192.168.20.24)
export K3S_TOKEN="<token-from-master>"
export K3S_URL="https://192.168.20.10:6443"

curl -sfL https://get.k3s.io | sh -s - agent \
  --server ${K3S_URL} \
  --token ${K3S_TOKEN} \
  --node-ip=192.168.20.24

# Label node for specific workloads
kubectl label node k3s-worker-4 workload=production
```

### Increase Storage Capacity
```bash
# For Longhorn volumes
# 1. Add disk to worker nodes
# 2. Format disk
mkfs.ext4 /dev/sdb

# 3. Add to Longhorn
kubectl -n longhorn-system edit node <node-name>
# Add disk path in spec.disks

# For NFS
# Expand underlying storage, then
df -h /export/k8s-pv
resize2fs /dev/sdb1  # If using ext4
```

## Backup and Restore

### Manual GitLab Backup
```bash
# Create backup
gitlab-backup create

# List backups
ls -lh /var/opt/gitlab/backups/

# Copy to safe location
rsync -av /var/opt/gitlab/backups/ user@backup-server:/backups/gitlab/
```

### Restore Specific GitLab Component
```bash
# Restore only repositories
gitlab-backup restore BACKUP=<timestamp> SKIP=db,uploads,builds,artifacts,lfs,terraform_state,registry,pages

# Restore only database
gitlab-backup restore BACKUP=<timestamp> SKIP=repositories,uploads,builds,artifacts,lfs,terraform_state,registry,pages
```

### PostgreSQL Point-in-Time Recovery
```bash
# Enable WAL archiving in postgresql.conf
archive_mode = on
archive_command = 'test ! -f /var/lib/postgresql/15/archive/%f && cp %p /var/lib/postgresql/15/archive/%f'

# Base backup
sudo -u postgres pg_basebackup -D /backup/pg-base -Ft -z -P

# PITR restore (if needed)
# 1. Stop PostgreSQL
# 2. Restore base backup
# 3. Create recovery.conf
# 4. Start PostgreSQL
```

### Kubernetes Namespace Backup
```bash
# Backup specific namespace
velero backup create namespace-backup \
  --include-namespaces production \
  --wait

# Schedule regular backups
velero schedule create production-daily \
  --schedule="0 2 * * *" \
  --include-namespaces production \
  --ttl 168h
```

## Performance Tuning

### GitLab Performance
```bash
# Check current settings
gitlab-rake gitlab:env:info

# Tune Puma workers (in /etc/gitlab/gitlab.rb)
puma['worker_processes'] = 4
puma['max_threads'] = 4

# Tune Sidekiq
sidekiq['max_concurrency'] = 25

# Apply changes
gitlab-ctl reconfigure
gitlab-ctl restart
```

### PostgreSQL Tuning
```bash
# Memory settings (/etc/postgresql/15/main/postgresql.conf)
shared_buffers = 4GB
effective_cache_size = 12GB
maintenance_work_mem = 1GB
work_mem = 16MB

# Connections
max_connections = 200

# Vacuum settings
autovacuum = on
autovacuum_max_workers = 3

# Apply changes
systemctl restart postgresql

# Monitor queries
sudo -u postgres psql -c "SELECT * FROM pg_stat_statements ORDER BY total_exec_time DESC LIMIT 10;"
```

### Kubernetes Resource Optimization
```bash
# Check resource usage
kubectl top nodes
kubectl top pods -A --sort-by=memory
kubectl top pods -A --sort-by=cpu

# Identify resource hogs
kubectl get pods -A -o custom-columns=NAME:.metadata.name,NAMESPACE:.metadata.namespace,CPU:.spec.containers[*].resources.requests.cpu,MEMORY:.spec.containers[*].resources.requests.memory

# Set resource quotas
cat << YAML | kubectl apply -f -
apiVersion: v1
kind: ResourceQuota
metadata:
  name: production-quota
  namespace: production
spec:
  hard:
    requests.cpu: "20"
    requests.memory: 40Gi
    limits.cpu: "40"
    limits.memory: 80Gi
YAML
```

### Longhorn Performance
```bash
# Check volume performance
kubectl -n longhorn-system get volumes

# Change replica count
kubectl -n longhorn-system edit volume <volume-name>
# Set numberOfReplicas: 2

# Monitor I/O
kubectl -n longhorn-system logs -f <longhorn-manager-pod>
```

## Emergency Procedures

### Complete Service Outage
```bash
# 1. Assess situation
/usr/local/bin/infrastructure-health-check.sh

# 2. Check HAProxy VIP
ip addr show | grep 192.168.30.1

# 3. Verify network connectivity
for vlan in 10 20 30; do
  echo "Testing VLAN $vlan"
  ping -c 2 192.168.$vlan.1
done

# 4. Check critical services
systemctl status haproxy keepalived
kubectl get nodes
gitlab-ctl status

# 5. Review logs
journalctl -xe
dmesg | tail -50
```

### GitLab Database Corruption
```bash
# 1. Stop GitLab
gitlab-ctl stop

# 2. Backup current state
cp -r /var/opt/gitlab/postgresql /var/opt/gitlab/postgresql.backup

# 3. Check database integrity
sudo -u gitlab-psql /opt/gitlab/embedded/bin/psql -h /var/opt/gitlab/postgresql -d gitlabhq_production -c "REINDEX DATABASE gitlabhq_production;"

# 4. If corruption severe, restore from backup
gitlab-backup restore BACKUP=<latest-good-backup>

# 5. Restart
gitlab-ctl start
gitlab-rake gitlab:check
```

### Kubernetes etcd Failure
```bash
# Check etcd status on all masters
for i in 10 11 12; do
  echo "Checking master $i"
  ssh 192.168.20.$i "systemctl status k3s"
done

# If etcd corrupted on one node
# 1. Stop K3s on failed node
systemctl stop k3s

# 2. Remove etcd data
rm -rf /var/lib/rancher/k3s/server/db

# 3. Rejoin cluster
systemctl start k3s

# 4. Verify etcd members
kubectl exec -n kube-system etcd-k3s-master-1 -- etcdctl member list
```

### Disk Space Critical
```bash
# Find large files
du -h / --max-depth=2 2>/dev/null | sort -h -r | head -20

# Clean Docker images (on K8s nodes)
crictl rmi --prune

# Clean old logs
journalctl --vacuum-time=7d
find /var/log -type f -name "*.log" -mtime +30 -delete

# Clean GitLab artifacts
gitlab-rake gitlab:cleanup:orphan_job_artifact_files

# Clean Nexus blobs
# Run Cleanup policies in Nexus UI

# Expand volumes if needed (LVM example)
lvextend -L +50G /dev/vg0/lv_root
resize2fs /dev/vg0/lv_root
```

### Network Partition
```bash
# Diagnose network issues
mtr -r 192.168.10.10  # GitLab
mtr -r 192.168.20.10  # K8s master
mtr -r 192.168.30.1   # VIP

# Check VLAN configuration
ip link show | grep vmbr
brctl show

# Verify routing
ip route show
iptables -L -n -v

# Check for split-brain in keepalived
# Should only see VIP on one node
for i in 10 11; do
  ssh 192.168.30.$i "ip addr show eth0 | grep 192.168.30.1"
done
```

## Performance Monitoring Commands

### Real-time Monitoring
```bash
# Node resources
watch -n 2 'kubectl top nodes'

# Pod resources
watch -n 2 'kubectl top pods -A --sort-by=memory | head -20'

# Service response times
watch -n 5 'curl -s -w "%{time_total}\n" -o /dev/null http://192.168.30.1'

# Database connections
watch -n 5 'sudo -u postgres psql -c "SELECT count(*) FROM pg_stat_activity;"'
```

### Log Aggregation Queries
```bash
# Recent errors in Loki
logcli query '{namespace="default"} |= "error"' --limit=50 --since=1h

# GitLab errors
tail -f /var/log/gitlab/gitlab-rails/production.log | grep ERROR

# HAProxy connection errors
journalctl -u haproxy -f | grep -i error
```

---

**Last Updated**: $(date)
**Maintained By**: DevOps Team
**Review Frequency**: Monthly
EOF

# Push documentation to GitLab
git init
git remote add origin https://gitlab.yourdomain.com/infrastructure/docs.git
git add .
git commit -m "Initial infrastructure documentation"
git push -u origin main
````

---

## Этап 11: Final Checklist и Go-Live

### 11.1 Pre-Production Checklist

bash

```bash
cat << 'EOF' > /tmp/production-checklist.md
# Production Readiness Checklist

## Infrastructure Components
- [ ] All VLANs configured and tested
- [ ] NFS storage mounted on all required nodes
- [ ] DNS records configured (if applicable)
- [ ] Firewall rules implemented
- [ ] Network policies tested

## Stateful Services
- [ ] PostgreSQL cluster operational and tested
- [ ] PostgreSQL backups running automatically
- [ ] GitLab installed and accessible
- [ ] GitLab backup cron job configured
- [ ] Nexus repository operational
- [ ] Nexus Docker registry configured
- [ ] SonarQube installed and configured
- [ ] All services accessible via HAProxy VIP

## High Availability
- [ ] HAProxy VRRP failover tested
- [ ] Keepalived VIP transitions smoothly
- [ ] K3s 3-master HA operational
- [ ] K3s API accessible via all masters
- [ ] etcd cluster health verified
- [ ] Worker node failure handled gracefully

## Kubernetes Cluster
- [ ] All 6 nodes (3 masters + 3 workers) joined
- [ ] MetalLB IP pool configured (192.168.20.100-150)
- [ ] Ingress Nginx controller deployed
- [ ] Longhorn storage operational
- [ ] StorageClass set as default
- [ ] Persistent volumes can be created

## CI/CD Pipeline
- [ ] GitLab Runner registered and operational
- [ ] Test pipeline executed successfully
- [ ] PetClinic builds successfully
- [ ] Docker images push to Nexus
- [ ] SonarQube analysis runs
- [ ] Kubernetes deployment successful
- [ ] Application accessible via Ingress

## Monitoring & Logging
- [ ] Prometheus stack installed
- [ ] Grafana dashboards configured
- [ ] AlertManager configured
- [ ] Loki log aggregation operational
- [ ] Promtail collecting logs from all pods
- [ ] Node exporters on all infrastructure VMs
- [ ] ServiceMonitors configured for apps

## Security
- [ ] SSL/TLS certificates configured
- [ ] cert-manager operational
- [ ] Network policies implemented
- [ ] Pod Security Standards enforced
- [ ] Sealed Secrets controller installed
- [ ] RBAC roles configured
- [ ] Service accounts created
- [ ] Secrets properly managed

## Backup & DR
- [ ] GitLab backups automated (daily 2 AM)
- [ ] Nexus backups automated (daily 3 AM)
- [ ] PostgreSQL backups automated (daily 1 AM)
- [ ] Velero installed and configured
- [ ] MinIO backup target operational
- [ ] K8s daily backups scheduled
- [ ] Backup retention policies set (7 days)
- [ ] Restore procedures tested

## Performance & Capacity
- [ ] Resource requests/limits set for all pods
- [ ] HPA configured for scalable apps
- [ ] PostgreSQL tuned for workload
- [ ] GitLab Puma/Sidekiq optimized
- [ ] Longhorn replica count appropriate
- [ ] Load testing performed
- [ ] Baseline metrics established

## Documentation
- [ ] Architecture diagram updated
- [ ] README.md comprehensive
- [ ] RUNBOOK.md created
- [ ] Service inventory documented
- [ ] Access credentials documented (secure location)
- [ ] Escalation procedures defined
- [ ] Disaster recovery plan complete

## Testing
- [ ] Health check script runs successfully
- [ ] HAProxy failover tested
- [ ] K8s master failover tested
- [ ] Storage failover tested
- [ ] Application deployment tested
- [ ] Load testing completed
- [ ] Backup/restore tested

## Go-Live Preparation
- [ ] Change management approval obtained
- [ ] Rollback plan documented
- [ ] Maintenance window scheduled
- [ ] Team trained on operations
- [ ] Monitoring alerts configured
- [ ] On-call rotation established
- [ ] Communication plan ready

---

**Signed Off By:**
- Infrastructure Lead: ___________ Date: ___________
- Development Lead: ___________ Date: ___________
- Security Lead: ___________ Date: ___________
- Management: ___________ Date: ___________

EOF

# Verify checklist items programmatically
cat << 'EOF' > /usr/local/bin/verify-production-readiness.sh
#!/bin/bash

echo "========================================"
echo "Production Readiness Verification"
echo "========================================"
echo ""

passed=0
failed=0

# Function to test and report
test_component() {
  local name=$1
  local command=$2
  
  if eval "$command" > /dev/null 2>&1; then
    echo "✓ $name"
    ((passed++))
  else
    echo "✗ $name"
    ((failed++))
  fi
}

# Infrastructure tests
echo "Infrastructure Components:"
test_component "HAProxy VIP accessible" "ping -c 1 -W 2 192.168.30.1"
test_component "NFS server accessible" "showmount -e 192.168.10.50"
test_component "PostgreSQL accessible" "pg_isready -h 192.168.10.40"

# Services tests
echo ""
echo "Stateful Services:"
test_component "GitLab responding" "curl -s -o /dev/null -w '%{http_code}' http://192.168.30.1 | grep -q 302"
test_component "Nexus responding" "curl -s -o /dev/null -w '%{http_code}' http://192.168.30.1:8081 | grep -q 200"
test_component "SonarQube responding" "curl -s -o /dev/null -w '%{http_code}' http://192.168.30.1:9000 | grep -q 200"

# Kubernetes tests
echo ""
echo "Kubernetes Cluster:"
test_component "K8s API accessible" "kubectl cluster-info"
test_component "All nodes ready" "[[ \$(kubectl get nodes --no-headers | grep -c Ready) -eq 6 ]]"
test_component "MetalLB operational" "kubectl get pods -n metallb-system -o jsonpath='{.items[*].status.phase}' | grep -q Running"
test_component "Ingress Nginx operational" "kubectl get pods -n ingress-nginx -o jsonpath='{.items[*].status.phase}' | grep -q Running"
test_component "Longhorn operational" "kubectl get pods -n longhorn-system -o jsonpath='{.items[*].status.phase}' | grep -q Running"

# CI/CD tests
echo ""
echo "CI/CD Pipeline:"
test_component "GitLab Runner registered" "kubectl get pods -n gitlab-runner -o jsonpath='{.items[*].status.phase}' | grep -q Running"
test_component "Nexus Docker registry" "curl -s http://192.168.10.20:8082/v2/_catalog"

# Monitoring tests
echo ""
echo "Monitoring & Logging:"
test_component "Prometheus operational" "kubectl get pods -n monitoring -l app.kubernetes.io/name=prometheus -o jsonpath='{.items[*].status.phase}' | grep -q Running"
test_component "Grafana operational" "kubectl get pods -n monitoring -l app.kubernetes.io/name=grafana -o jsonpath='{.items[*].status.phase}' | grep -q Running"
test_component "Loki operational" "kubectl get pods -n monitoring -l app.kubernetes.io/name=loki -o
```

Retry

A

Continue

bash

```bash
jsonpath='{.items[*].status.phase}' | grep -q Running"

# Backup tests
echo ""
echo "Backup & DR:"
test_component "GitLab backup script exists" "test -f /usr/local/bin/gitlab-backup.sh"
test_component "GitLab backup cron configured" "grep -q gitlab-backup /etc/cron.d/gitlab-backup"
test_component "Nexus backup script exists" "test -f /usr/local/bin/nexus-backup.sh"
test_component "PostgreSQL backup script exists" "test -f /usr/local/bin/postgres-backup.sh"
test_component "Velero installed" "kubectl get pods -n velero -o jsonpath='{.items[*].status.phase}' | grep -q Running"
test_component "Recent backup exists" "velero backup get | grep -q Completed"

# Security tests
echo ""
echo "Security:"
test_component "cert-manager operational" "kubectl get pods -n cert-manager -o jsonpath='{.items[*].status.phase}' | grep -q Running"
test_component "Sealed Secrets operational" "kubectl get pods -n kube-system -l name=sealed-secrets-controller -o jsonpath='{.items[*].status.phase}' | grep -q Running"
test_component "Network policies exist" "kubectl get networkpolicies -n default | grep -q deny-all-ingress"

echo ""
echo "========================================"
echo "Summary: $passed passed, $failed failed"
echo "========================================"

if [ $failed -eq 0 ]; then
  echo "✓ All checks passed - System is production ready!"
  exit 0
else
  echo "✗ Some checks failed - Review before going to production"
  exit 1
fi
EOF

chmod +x /usr/local/bin/verify-production-readiness.sh

# Run verification
/usr/local/bin/verify-production-readiness.sh
```

### 11.2 Performance Baseline Establishment

bash

```bash
# Создаем скрипт для сбора базовых метрик
cat << 'EOF' > /usr/local/bin/collect-baseline-metrics.sh
#!/bin/bash

OUTPUT_DIR="/tmp/baseline-metrics-$(date +%Y%m%d-%H%M%S)"
mkdir -p $OUTPUT_DIR

echo "Collecting baseline metrics..."
echo "Output directory: $OUTPUT_DIR"

# System metrics
echo "Collecting system metrics..."
kubectl top nodes > $OUTPUT_DIR/nodes-resources.txt
kubectl top pods -A > $OUTPUT_DIR/pods-resources.txt

# Service response times
echo "Collecting service response times..."
for service in "GitLab:http://192.168.30.1" "Nexus:http://192.168.30.1:8081" "SonarQube:http://192.168.30.1:9000"; do
  name=$(echo $service | cut -d: -f1)
  url=$(echo $service | cut -d: -f2-)
  
  echo "Testing $name..."
  for i in {1..10}; do
    curl -s -o /dev/null -w "%{time_total}\n" $url
  done | awk '{sum+=$1; count++} END {print "'$name' average:", sum/count, "seconds"}' >> $OUTPUT_DIR/response-times.txt
done

# Database connections
echo "Collecting database metrics..."
ssh 192.168.10.40 "sudo -u postgres psql -c 'SELECT * FROM pg_stat_database;'" > $OUTPUT_DIR/postgres-stats.txt
ssh 192.168.10.40 "sudo -u postgres psql -c 'SELECT * FROM pg_stat_activity;'" > $OUTPUT_DIR/postgres-connections.txt

# Storage metrics
echo "Collecting storage metrics..."
kubectl get pv -o json | jq '.items[] | {name: .metadata.name, capacity: .spec.capacity.storage, used: .status.phase}' > $OUTPUT_DIR/storage-volumes.json

# Longhorn metrics
kubectl -n longhorn-system get volumes -o json > $OUTPUT_DIR/longhorn-volumes.json

# Network metrics
echo "Collecting network metrics..."
kubectl get svc -A | grep LoadBalancer > $OUTPUT_DIR/loadbalancer-services.txt

# Create summary report
cat << SUMMARY > $OUTPUT_DIR/SUMMARY.md
# Baseline Metrics Report
Generated: $(date)

## Resource Utilization

### Node Resources
\`\`\`
$(cat $OUTPUT_DIR/nodes-resources.txt)
\`\`\`

### Top Resource Consumers
\`\`\`
$(head -20 $OUTPUT_DIR/pods-resources.txt)
\`\`\`

## Service Performance

### Response Times
\`\`\`
$(cat $OUTPUT_DIR/response-times.txt)
\`\`\`

## Storage Utilization

### Persistent Volumes
\`\`\`
$(kubectl get pv | grep Bound | wc -l) volumes bound
$(kubectl get pv | grep Available | wc -l) volumes available
\`\`\`

## Database Metrics

### PostgreSQL Active Connections
\`\`\`
$(ssh 192.168.10.40 "sudo -u postgres psql -t -c 'SELECT count(*) FROM pg_stat_activity;'" 2>/dev/null)
\`\`\`

## Recommendations

1. Monitor trends over the next 30 days
2. Set up alerts for 80% resource utilization
3. Plan capacity upgrades when trending above 70%
4. Review and optimize top resource consumers

---
Next baseline collection: $(date -d '+30 days')
SUMMARY

echo ""
echo "Baseline metrics collected successfully!"
echo "Report available at: $OUTPUT_DIR/SUMMARY.md"
echo ""
echo "Archive this report for future comparison"

# Compress and archive
tar -czf $OUTPUT_DIR.tar.gz $OUTPUT_DIR
echo "Archived to: $OUTPUT_DIR.tar.gz"
EOF

chmod +x /usr/local/bin/collect-baseline-metrics.sh

# Run baseline collection
/usr/local/bin/collect-baseline-metrics.sh
```

### 11.3 Monitoring Alerts Configuration

bash

```bash
# Создаем AlertManager configuration
cat << 'EOF' > alertmanager-config.yaml
apiVersion: v1
kind: Secret
metadata:
  name: alertmanager-prometheus-kube-prometheus-alertmanager
  namespace: monitoring
type: Opaque
stringData:
  alertmanager.yaml: |
    global:
      resolve_timeout: 5m
      
    route:
      group_by: ['alertname', 'cluster', 'service']
      group_wait: 10s
      group_interval: 10s
      repeat_interval: 12h
      receiver: 'default'
      routes:
      - match:
          severity: critical
        receiver: 'critical-alerts'
        continue: true
      - match:
          severity: warning
        receiver: 'warning-alerts'
    
    receivers:
    - name: 'default'
      webhook_configs:
      - url: 'http://localhost:5001/'
        send_resolved: true
    
    - name: 'critical-alerts'
      email_configs:
      - to: 'devops-oncall@yourdomain.com'
        from: 'alerts@yourdomain.com'
        smarthost: 'smtp.gmail.com:587'
        auth_username: 'alerts@yourdomain.com'
        auth_password: 'your-app-password'
        headers:
          Subject: '[CRITICAL] {{ .GroupLabels.alertname }}'
      # Uncomment if you have Slack
      # slack_configs:
      # - api_url: 'YOUR_SLACK_WEBHOOK_URL'
      #   channel: '#infrastructure-alerts'
      #   title: 'CRITICAL: {{ .GroupLabels.alertname }}'
      #   text: '{{ range .Alerts }}{{ .Annotations.description }}{{ end }}'
    
    - name: 'warning-alerts'
      email_configs:
      - to: 'devops@yourdomain.com'
        from: 'alerts@yourdomain.com'
        smarthost: 'smtp.gmail.com:587'
        auth_username: 'alerts@yourdomain.com'
        auth_password: 'your-app-password'
        headers:
          Subject: '[WARNING] {{ .GroupLabels.alertname }}'
EOF

kubectl apply -f alertmanager-config.yaml

# Создаем PrometheusRule для critical alerts
cat << 'EOF' | kubectl apply -f -
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: infrastructure-alerts
  namespace: monitoring
  labels:
    prometheus: kube-prometheus
spec:
  groups:
  - name: infrastructure
    interval: 30s
    rules:
    # Node alerts
    - alert: NodeDown
      expr: up{job="node-exporter"} == 0
      for: 5m
      labels:
        severity: critical
      annotations:
        summary: "Node {{ $labels.instance }} is down"
        description: "Node {{ $labels.instance }} has been down for more than 5 minutes."
    
    - alert: NodeHighCPU
      expr: 100 - (avg by(instance) (irate(node_cpu_seconds_total{mode="idle"}[5m])) * 100) > 85
      for: 10m
      labels:
        severity: warning
      annotations:
        summary: "High CPU usage on {{ $labels.instance }}"
        description: "CPU usage is above 85% for 10 minutes on {{ $labels.instance }}"
    
    - alert: NodeHighMemory
      expr: (1 - (node_memory_MemAvailable_bytes / node_memory_MemTotal_bytes)) * 100 > 90
      for: 5m
      labels:
        severity: critical
      annotations:
        summary: "High memory usage on {{ $labels.instance }}"
        description: "Memory usage is above 90% on {{ $labels.instance }}"
    
    - alert: NodeDiskSpaceLow
      expr: (1 - (node_filesystem_avail_bytes / node_filesystem_size_bytes)) * 100 > 85
      for: 5m
      labels:
        severity: warning
      annotations:
        summary: "Low disk space on {{ $labels.instance }}"
        description: "Disk {{ $labels.mountpoint }} is {{ $value }}% full on {{ $labels.instance }}"
    
    - alert: NodeDiskSpaceCritical
      expr: (1 - (node_filesystem_avail_bytes / node_filesystem_size_bytes)) * 100 > 95
      for: 5m
      labels:
        severity: critical
      annotations:
        summary: "Critical disk space on {{ $labels.instance }}"
        description: "Disk {{ $labels.mountpoint }} is {{ $value }}% full on {{ $labels.instance }}"
    
    # Kubernetes alerts
    - alert: KubernetesPodCrashLooping
      expr: rate(kube_pod_container_status_restarts_total[15m]) > 0
      for: 5m
      labels:
        severity: warning
      annotations:
        summary: "Pod {{ $labels.namespace }}/{{ $labels.pod }} is crash looping"
        description: "Pod {{ $labels.namespace }}/{{ $labels.pod }} is restarting frequently"
    
    - alert: KubernetesPodNotReady
      expr: kube_pod_status_phase{phase!~"Running|Succeeded"} > 0
      for: 10m
      labels:
        severity: warning
      annotations:
        summary: "Pod {{ $labels.namespace }}/{{ $labels.pod }} not ready"
        description: "Pod {{ $labels.namespace }}/{{ $labels.pod }} has been in {{ $labels.phase }} state for more than 10 minutes"
    
    - alert: KubernetesNodeNotReady
      expr: kube_node_status_condition{condition="Ready",status="true"} == 0
      for: 5m
      labels:
        severity: critical
      annotations:
        summary: "Kubernetes node {{ $labels.node }} not ready"
        description: "Node {{ $labels.node }} has been not ready for more than 5 minutes"
    
    - alert: KubernetesPersistentVolumeClaimPending
      expr: kube_persistentvolumeclaim_status_phase{phase="Pending"} > 0
      for: 10m
      labels:
        severity: warning
      annotations:
        summary: "PVC {{ $labels.namespace }}/{{ $labels.persistentvolumeclaim }} pending"
        description: "PVC {{ $labels.namespace }}/{{ $labels.persistentvolumeclaim }} has been pending for more than 10 minutes"
    
    # Application alerts
    - alert: ApplicationDown
      expr: up{job="kubernetes-pods"} == 0
      for: 5m
      labels:
        severity: critical
      annotations:
        summary: "Application {{ $labels.pod }} is down"
        description: "Application pod {{ $labels.namespace }}/{{ $labels.pod }} has been down for more than 5 minutes"
    
    - alert: HighErrorRate
      expr: rate(http_requests_total{status=~"5.."}[5m]) > 0.05
      for: 5m
      labels:
        severity: warning
      annotations:
        summary: "High error rate for {{ $labels.job }}"
        description: "Error rate is {{ $value }} for {{ $labels.job }}"
    
    - alert: SlowResponseTime
      expr: histogram_quantile(0.95, rate(http_request_duration_seconds_bucket[5m])) > 1
      for: 10m
      labels:
        severity: warning
      annotations:
        summary: "Slow response time for {{ $labels.job }}"
        description: "95th percentile response time is {{ $value }}s for {{ $labels.job }}"
    
    # Storage alerts
    - alert: LonghornVolumeUsageHigh
      expr: (longhorn_volume_actual_size_bytes / longhorn_volume_capacity_bytes) * 100 > 85
      for: 5m
      labels:
        severity: warning
      annotations:
        summary: "Longhorn volume {{ $labels.volume }} usage high"
        description: "Volume {{ $labels.volume }} is {{ $value }}% full"
    
    # Database alerts
    - alert: PostgreSQLDown
      expr: pg_up == 0
      for: 5m
      labels:
        severity: critical
      annotations:
        summary: "PostgreSQL is down"
        description: "PostgreSQL instance {{ $labels.instance }} has been down for more than 5 minutes"
    
    - alert: PostgreSQLTooManyConnections
      expr: sum(pg_stat_activity_count) > 180
      for: 5m
      labels:
        severity: warning
      annotations:
        summary: "PostgreSQL has too many connections"
        description: "PostgreSQL has {{ $value }} active connections (threshold: 180)"
EOF
```

### 11.4 Final Configuration Export

bash

```bash
# Создаем скрипт для экспорта всей конфигурации
cat << 'EOF' > /usr/local/bin/export-infrastructure-config.sh
#!/bin/bash

EXPORT_DIR="/tmp/infrastructure-config-$(date +%Y%m%d-%H%M%S)"
mkdir -p $EXPORT_DIR/{kubernetes,gitlab,haproxy,monitoring,backups}

echo "Exporting infrastructure configuration..."
echo "Export directory: $EXPORT_DIR"

# Kubernetes resources
echo "Exporting Kubernetes resources..."
for ns in default kube-system monitoring ingress-nginx longhorn-system gitlab-runner velero metallb-system cert-manager; do
  if kubectl get namespace $ns &>/dev/null; then
    mkdir -p $EXPORT_DIR/kubernetes/$ns
    kubectl get all -n $ns -o yaml > $EXPORT_DIR/kubernetes/$ns/all-resources.yaml
    kubectl get cm -n $ns -o yaml > $EXPORT_DIR/kubernetes/$ns/configmaps.yaml
    kubectl get secrets -n $ns -o yaml > $EXPORT_DIR/kubernetes/$ns/secrets.yaml
    kubectl get pvc -n $ns -o yaml > $EXPORT_DIR/kubernetes/$ns/pvcs.yaml
    kubectl get ingress -n $ns -o yaml > $EXPORT_DIR/kubernetes/$ns/ingress.yaml 2>/dev/null || true
  fi
done

# StorageClasses and PVs
kubectl get storageclass -o yaml > $EXPORT_DIR/kubernetes/storageclasses.yaml
kubectl get pv -o yaml > $EXPORT_DIR/kubernetes/persistent-volumes.yaml

# GitLab configuration
echo "Exporting GitLab configuration..."
scp 192.168.10.10:/etc/gitlab/gitlab.rb $EXPORT_DIR/gitlab/ 2>/dev/null || echo "Could not export GitLab config"
scp 192.168.10.10:/etc/gitlab/gitlab-secrets.json $EXPORT_DIR/gitlab/ 2>/dev/null || echo "Could not export GitLab secrets"

# HAProxy configuration
echo "Exporting HAProxy configuration..."
scp 192.168.30.10:/etc/haproxy/haproxy.cfg $EXPORT_DIR/haproxy/haproxy-1.cfg 2>/dev/null || true
scp 192.168.30.11:/etc/haproxy/haproxy.cfg $EXPORT_DIR/haproxy/haproxy-2.cfg 2>/dev/null || true
scp 192.168.30.10:/etc/keepalived/keepalived.conf $EXPORT_DIR/haproxy/keepalived-1.conf 2>/dev/null || true
scp 192.168.30.11:/etc/keepalived/keepalived.conf $EXPORT_DIR/haproxy/keepalived-2.conf 2>/dev/null || true

# Monitoring configuration
echo "Exporting monitoring configuration..."
kubectl get prometheus -n monitoring -o yaml > $EXPORT_DIR/monitoring/prometheus.yaml
kubectl get alertmanager -n monitoring -o yaml > $EXPORT_DIR/monitoring/alertmanager.yaml
kubectl get prometheusrule -n monitoring -o yaml > $EXPORT_DIR/monitoring/prometheus-rules.yaml
kubectl get servicemonitor -n monitoring -o yaml > $EXPORT_DIR/monitoring/service-monitors.yaml

# Backup scripts
echo "Exporting backup scripts..."
cp /usr/local/bin/gitlab-backup.sh $EXPORT_DIR/backups/ 2>/dev/null || true
cp /usr/local/bin/nexus-backup.sh $EXPORT_DIR/backups/ 2>/dev/null || true
cp /usr/local/bin/postgres-backup.sh $EXPORT_DIR/backups/ 2>/dev/null || true

# Documentation
echo "Exporting documentation..."
cp /tmp/infrastructure-docs/README.md $EXPORT_DIR/ 2>/dev/null || true
cp /tmp/infrastructure-docs/RUNBOOK.md $EXPORT_DIR/ 2>/dev/null || true

# System information
echo "Collecting system information..."
cat << INFO > $EXPORT_DIR/SYSTEM_INFO.txt
Export Date: $(date)
Export By: $(whoami)

Kubernetes Version: $(kubectl version --short 2>/dev/null | head -2)

Nodes:
$(kubectl get nodes)

Pods Summary:
$(kubectl get pods -A | wc -l) total pods

Storage:
$(kubectl get pv | wc -l) persistent volumes
$(kubectl get pvc -A | wc -l) persistent volume claims

Services:
GitLab: $(curl -s -o /dev/null -w '%{http_code}' http://192.168.30.1)
Nexus: $(curl -s -o /dev/null -w '%{http_code}' http://192.168.30.1:8081)
SonarQube: $(curl -s -o /dev/null -w '%{http_code}' http://192.168.30.1:9000)
INFO

# Create archive
echo "Creating archive..."
cd /tmp
tar -czf infrastructure-config-$(date +%Y%m%d-%H%M%S).tar.gz $(basename $EXPORT_DIR)

echo ""
echo "Export completed successfully!"
echo "Archive: /tmp/infrastructure-config-$(date +%Y%m%d-%H%M%S).tar.gz"
echo ""
echo "IMPORTANT: This archive contains sensitive information."
echo "Store it securely and encrypt if storing remotely."
EOF

chmod +x /usr/local/bin/export-infrastructure-config.sh

# Run export
/usr/local/bin/export-infrastructure-config.sh
```

### 11.5 Go-Live Procedure

bash

```bash
# Финальный go-live checklist script
cat << 'EOF' > /usr/local/bin/go-live-final-check.sh
#!/bin/bash

echo "=========================================="
echo "FINAL GO-LIVE VERIFICATION"
echo "=========================================="
echo ""
echo "Date: $(date)"
echo "Operator: $(whoami)"
echo ""

# Prompt for confirmation
read -p "Have you completed the production readiness checklist? (yes/no): " checklist
if [ "$checklist" != "yes" ]; then
  echo "Please complete the checklist first"
  exit 1
fi

read -p "Have backups been verified? (yes/no): " backups
if [ "$backups" != "yes" ]; then
  echo "Please verify backups first"
  exit 1
fi

read -p "Has the team been notified? (yes/no): " team
if [ "$team" != "yes" ]; then
  echo "Please notify the team first"
  exit 1
fi

echo ""
echo "Running final verification tests..."
echo ""

# Run comprehensive health check
/usr/local/bin/infrastructure-health-check.sh

# Run production readiness verification
/usr/local/bin/verify-production-readiness.sh

if [ $? -ne 0 ]; then
  echo ""
  echo "❌ VERIFICATION FAILED - DO NOT PROCEED WITH GO-LIVE"
  exit 1
fi

echo ""
echo "Collecting baseline metrics..."
/usr/local/bin/collect-baseline-metrics.sh

echo ""
echo "Exporting configuration backup..."
/usr/local/bin/export-infrastructure-config.sh

echo ""
echo "=========================================="
echo "✅ ALL PRE-FLIGHT CHECKS PASSED"
echo "=========================================="
echo ""
echo "System is ready for production!"
echo ""
echo "Post Go-Live Tasks:"
echo "1. Monitor dashboards for 24 hours"
echo "2. Verify alerts are working"
echo "3. Test application deployment"
echo "4. Verify user access"
echo "5. Document any issues"
echo ""
echo "Emergency Contacts:"
echo "- On-call DevOps: <phone-number>"
echo "- Infrastructure Lead: <phone-number>"
echo "- Escalation: <phone-number>"
echo ""

read -p "Proceed with GO-LIVE? (yes/no): " golive
if [ "$golive" == "yes" ]; then
  echo ""
  echo "🚀 SYSTEM IS NOW IN PRODUCTION"
  echo ""
  echo "$(date): Production go-live completed by $(whoami)" >> /var/log/production-events.log
  
  # Send notification (customize based on your notification system)
  # curl -X POST https://your-webhook-url -d "Production infrastructure is now live"
  
  echo "Go-live logged to /var/log/production-events.log"
else
  echo "Go-live cancelled"
  exit 0
fi
EOF

chmod +x /usr/local/bin/go-live-final-check.sh
```

---

## Post-Production Operations

### Daily Operations

bash

```bash
# Создаем скрипт для ежедневных проверок
cat << 'EOF' > /usr/local/bin/daily-health-check.sh
#!/bin/bash

# Run at 9 AM daily via cron
REPORT_FILE="/tmp/daily-health-$(date +%Y%m%d).txt"

{
  echo "=========================================="
  echo "Daily Health Report - $(date)"
  echo "=========================================="
  echo ""
  
  echo "Infrastructure Status:"
  /usr/local/bin/infrastructure-health-check.sh
  
  echo ""
  echo "Backup Status:"
  echo "Last GitLab backup: $(ls -t /var/opt/gitlab/backups/*.tar 2>/dev/null | head -1)"
  echo "Last Velero backup: $(velero backup get | grep Completed | head -1)"
  
  echo ""
  echo "Resource Usage:"
  kubectl top nodes
  
  echo ""
  echo "Top Resource Consumers:"
  kubectl top pods -A --sort-by=memory | head -10
  
  echo ""
  echo "Recent Alerts:"
  kubectl logs -n monitoring --tail=20 deployment/alertmanager-prometheus-kube-prometheus-alertmanager | grep -i firing || echo "No active alerts"
  
  echo ""
  echo "Storage Status:"
  kubectl get pv | grep -v Available | tail -n +2
  
  echo ""
  echo "=========================================="
} > $REPORT_FILE

# Email report (customize for your email setup)
# mail -s "Daily Infrastructure Health Report" devops@yourdomain.com < $REPORT_FILE

cat $REPORT_FILE
EOF

chmod +x /usr/local/bin/daily-health-check.sh

# Add to cron
echo "0 9 * * * root /usr/local/bin/daily-health-check.sh" > /etc/cron.d/daily-health-check
```

---

## Summary

Эта production-ready инфраструктура включает:

### ✅ Реализованные компоненты:

1. **Network Segmentation**: 4 VLAN для изоляции сервисов
2. **High Availability**:
    - HAProxy + Keepalived (Active/Passive)
    - K3s с 3 master нодами (embedded etcd)
    - PostgreSQL с репликацией
3. **Stateful Services** на выделенных VM:
    - GitLab (CI/CD + SCM)
    - Nexus (Artifacts + Docker Registry)
    - SonarQube (Code Quality)
    - PostgreSQL (Shared Database)
4. **Kubernetes Cluster**:
    - 3 masters + 3 workers
    - MetalLB LoadBalancer
    - Ingress Nginx
    - Longhorn Storage
5. **Monitoring Stack**:
    - Prometheus + Grafana
    - Loki + Promtail
    - AlertManager
    - Node Exporters
6. **Security**:
    - Network Policies
    - Pod Security Standards
    - Sealed Secrets
    - TLS/SSL (cert-manager)
7. **Backup & DR**:
    - Automated daily backups
    - Velero для K8s
    - 7-day retention
    - Disaster recovery procedures
8. **Documentation**:
    - Architecture diagrams
    - Runbooks
    - Troubleshooting guides
    - Go-live procedures

### 📊 Key Metrics:

- **Availability Target**: 99.9% uptime
- **RTO**: < 4 hours
- **RPO**: < 24 hours
- **Backup Retention**: 7 days
- **Monitoring Coverage**: 100% infrastructure

### 🔄 Continuous Improvement:

- Monthly maintenance windows
- Quarterly disaster recovery drills
- Regular security audits
- Capacity planning reviews


